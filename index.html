<html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1">

<title>Semantic Web Challenge on Tabular Data to Knowledge Graph Matching</title>
<link rel="stylesheet" type="text/css" href="style.css">
</head><body>

<div class="header">

 <!--
<div class="yellowbar">
<b>News (09/03/2022):</b> The <a href="http://ceur-ws.org/Vol-3103/" target="blank">SemTab 2021 proceedings</a> are out. <a href="#results">Results</a> and <a href="#gt">ground</a> truths are available. Back to <a href="../index.html">other SemTab editions</a>. 
</div>
-->

<!-- a style="color: grey; line-height: 5mm;" href="https://www.turing.ac.uk/research/research-projects/artificial-intelligence-data-analytics">Artificial Intelligence for Data Analytics</a-->
 
  <a href="http://sirius-labs.no/" target="blank">
  <img src="logos/sirius-logo.png" style="float: right; margin-top: 10pt; margin-right: 20pt; margin-left: 30pt; margin-bottom: 10pt; border-style: none;" width="175px"></a>
  
  <a href="http://www.cs.ox.ac.uk/" target="blank">
  <img src="logos/cs-oxford.jpg" style="clear: right; float: right; margin-right: 20pt; margin-left: 30pt; margin-bottom: 10pt; border-style: none;" width="175px"></a>
  
  <a href="https://www.city.ac.uk/about/schools/mathematics-computer-science-engineering/computer-science" target="blank">
  <img src="logos/city-logo.jpg" style="clear: right; float: right; margin-right: 20pt; margin-left: 30pt; margin-bottom: 10pt; border-style: none;" width="150px"></a>
 
 <a href="https://iswc2021.semanticweb.org/" target="blank">
  <img src="logos/iswc2022.png" style="clear: right; float: right; margin-right: 20pt; margin-left: 30pt; margin-bottom: 10pt; border-style: none;" width="175px"></a>
   
 
 
</div>



<h1>SemTab 2022: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching</h1>

<p>
Tabular data in the form of CSV files is the common input format in a data analytics pipeline. 
However a lack of understanding  of the semantic structure and meaning of the content may hinder 
the data analytics process. Thus gaining this semantic understanding will be very valuable for data integration, 
data cleaning, data mining, machine learning and knowledge discovery tasks. For example, understanding what the data 
is can help assess what sorts of transformation are appropriate on the data.
</p>

<p>Tables on the Web may also be the source of highly valuable data. The addition of semantic information to Web tables 
may enhance a wide range of applications, such as web search, question answering, and knowledge base (KB) construction.</p>


<p>Tabular data to Knowledge Graph (KG) matching is the process of assigning semantic tags from Knowledge Graphs (e.g., Wikidata or DBpedia) 
to the elements of the table.
This task however is often difficult in practice due to metadata (e.g., table and column names) being missing, incomplete or ambiguous.</p>

<p>
The <a href="http://www.cs.ox.ac.uk/isg/challenges/sem-tab/">SemTab challenge</a> aims at benchmarking systems dealing with the tabular data to KG matching problem, so as to
facilitate their comparison on the same basis and the reproducibility of the results. 
</p>


<p>The <b>2022 edition</b> of this challenge will be collocated with the <a href="https://iswc2022.semanticweb.org/" target="blank">21st International Semantic Web Conference</a> 
and the <a href="http://om2022.ontologymatching.org/" target="blanl">17th International Workshop on Ontology Matching</a>.
</p>


<br>

<a name="forum"></a>
<h2>Participation: forum and registration</h2>

<p>We have a <a href="https://groups.google.com/d/forum/sem-tab-challenge" target="blank">discussion group</a> for the challenge where we share the latest news with the participants 
and we discuss issues risen during the evaluation rounds.</p>

<p>Please register your system using this <a href="https://bit.ly/semtab2022-participation" target="blank">google form</a>.</p>

<p>Note that participants can join SemTab at any Round for any of the tasks/tracks.</p>

<br>

<a name="tasks"></a>
<h2>Challenge Tracks</h2>

<h3>Accuracy Track</h3>

The evaluation of systems regarding accuracy is similar to prior versions of the SemTab.<br>
That is, to illustrate the accuracy of the submissions, we evaluate systems on typical multi-class classification metrics as detailed below.<br>
In addition, we adopt the "cscore" for the CTA task to reflect the distance in the type hierarchy between the predicted column type and the ground truth semantic type.<br>
We use AICrowd to collect the submissions and automatically conduct the evaluation, the page on AICrowd will provide more details regarding evaluation.<br>
<br>
Matching Tasks:
<ul>
<li><b>CTA Task</b>: Assigning a semantic type (a DBpedia class as fine-grained as possible) to a column.</li>
<li><b>CEA Task</b>: Matching a cell to a Wikidata entity.</li>
<li><b>CPA Task</b>: Assigning a KG property to the relationship between two columns.</li>
</ul>


Matching Criteria:
<ul>
  <li>Average Precision</li>
  <li>Average Recall</li>
  <li>Average F1</li>
  <li>Cscore</li>
</ul>


Important Dates (all 2022):
<ul>
<li><b>May 26:</b> First call for challenge participants.</li>
<li><b>June 13 - July 13:</b> Round 1. </li>
<li><b>July 15 - August 14:</b> Round 2. </li>
<li><b>August 15:</b> Inivations to present at the <a href="https://iswc2022.semanticweb.org/" target="blank">ISWC conference</a>.
<li><b>September 15 - October 15:</b> Round 3.</li>
<li><b>October 21</b>: Paper submissions (via easychair), and artifact publication.</li>
<li><b>October 23 - 27</b>: Challenge presentation during OM workshop.</li>
<li><b>October 23 - 27</b>: Challenge Presentation and prize announcement during ISWC.</li>
<li><b>November 15</b>: Final version papers (via easychair).</li>
</ul>



<h3>Datasets Track</h3>

The data that table-to-Knowledge-Graph matching systems are trained and evaluated on, is critical for their accuracy and relevance.<br>
We invite dataset submissions that provide challenging and accessible new datasets to advance the state-of-the-art of table-to-KG matching systems.<br>
Preferably, these datasets provide tables along with their ground truth annotations for the CEA, CTA and CPA tasks.<br>
The dataset may be general or specific to a certain domain.<br>
<br>
Submissions will be evaluated according to provide the following:
<ul>
  <li>Description of the data collection, curation, and annotation processes.</li>
  <li>Availability of documentation with insights in the dataset content.</li>
  <li>Publicly accessible link to the dataset (e.g. Zenodo) and its DOI.</li>
  <li>Explanation of maintenance and long-term availability.</li>
  <li>Clear description of the envisioned use-cases.</li>
  <li>Application in which the dataset is used to solve an exemplar task.</li>
</ul>
 
We ask participants to describe their datasets submissions via easychair in a short paper (max 6 pages) that discusses how the above criteria are covered, while also including a link to the resources. 
The link to the resources may be private, until the submission is evaluated by the SemTab organisers. See paper guidelines below, for more details.

More guidance for creating, documenting and publishing datasets can be found <a href="https://neurips.cc/Conferences/2022/CallForDatasetsBenchmarks" target="blank">here</a>.

<br>
<br>
Important Dates (all 2022, tentative):
<ul>
<li><b>August 15:</b> Paper submissions (via easychair), and artifact publication.</li>
<li><b>September 30:</b> Notification of accept/reject.</li>
<li><b>October 23 - 27</b>: Dataset Presentation and prize announcement during ISWC.</li>
<li><b>November 15</b>: Final version papers (via easychair).</li>
</ul>



<h3>Artifacts Availability Badge</h3>

New this year is the Artifacts Availability Badge which is applicable to the Accuracy Track as well as the Datasets Track.<br>
The goal of this badge is to motivate authors to publish and document their systems, code, and data, so that others can use these artifacts and potentially reproduce or build on the results.<br>
This badge is given if all resources are verified to satisfy the below criteria.<br>
<br>
The criteria used to assess submissions (both accuracy and dataset submissions) are:
<ul>
  <li>Publicly accessible data (if applicable).</li>
  <li>Publicly accessible source code.</li>
  <li>Clear documentation of the code and data.</li>
  <li>Open-source dependencies.</li>
</ul>

<br>


<!-- BELOW FROM 2020: -->

<!-- <p>
The challenge will be run with the support of the 
<a href="https://www.aicrowd.com/challenges/semtab-2021" target="blank">AICrowd platform</a> and the <a href="https://bitbucket.org/disco_unimib/stiltool/" target="blank">STILTool system</a>.</p>
 -->

<!-- <h3>Datasets and tasks per round</h3>

<h4>Round 1:</h4> 
<ul>
<li><b>Knowledge Graphs:</b> DBPedia (version: <a href="http://downloads.dbpedia.org/wiki-archive/downloads-2016-10.html">2016-10</a>) and Wikidata (version: <a href="https://doi.org/10.5281/zenodo.6153449">20210828</a>)</li>
<li><b>Datasets and targets:</b> <a href="./data/tables_CTA_CEA_Round1.tar.gz" target="blank">tables of CTA-DBP and CEA-DBP</a>, <a href="./data/CTA_DBP_Round1_Targets.csv" target="blank">CTA-DBP targets</a>, <a href="./data/CEA_DBP_Round1_Targets.csv" target="blank">CEA-DBP targets</a>, <a href="./data/tables_CTA_CEA_WD_Round1.tar.gz" target="blank">tables of CTA-WD and CEA-WD</a>, <a href="./data/CTA_WD_Round1_Targets.csv">CTA-WD targets</a>, <a href="./data/CEA_WD_Round1_Targets.csv">CEA-WD targets</a>.</li>
<li><b>CTA-DBP Task</b>: Assigning a DBPedia semantic type (a DBpedia class as fine-grained as possible) to a column. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/column-type-annotation-by-dbpedia-cta-dbp">See AIcrowd page</a>.</li>
<li><b>CEA-DBP Task</b>: Matching a cell to a DBpedia entity. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/cell-entity-annotation-by-dbpedia-cea-dbp">See AIcrowd page</a>.</li>
<li><b>CTA-WD Task</b>: Assigning a Wikidata semantic type (a Wikidata entity as fine-grained as possible) to a column. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/column-type-annotation-by-wikidata-cta-wd">See AIcrowd page</a>.</li></li>
<li><b>CEA-WD Task</b>: Matching a cell to a Wikidata entity. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/cell-entity-annotation-by-wikidata-cea-wd">See AIcrowd page</a></li>
</ul>

<br>

<h4>Round 2: </h4>
<ul>
<li><b>Knowledge Graphs:</b> Wikidata (version: <a href="https://doi.org/10.5281/zenodo.6153449">20210828</a>)</li>
<li><b>BioTable Datasets and targets:</b> <a href="./data/BioTable_CTA_CEA_CPA_WD_Round2.tar.gz" target="blank">tables of BioTable-CTA-WD, BioTable-CEA-WD and BioTable-CPA-WD</a>, <a href="./data/BioTable_CTA_WD_Round2_Targets.csv" target="blank">BioTable-CTA-WD targets</a>, <a href="./data/BioTable_CEA_WD_Round2_Targets.csv" target="blank">BioTable-CEA-WD targets</a>, <a href="./data/BioTable_CPA_WD_Round2_Targets.csv" target="blank">BioTable-CPA-WD targets</a>.</li>
<li><b>BioTable-CTA-WD Task</b>: Assigning a Wikidata semantic type (a Wikidata entity as fine-grained as possible) to a column. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/biotable-column-type-annotation-by-wikidata-biotable-cta-wd">See AIcrowd page</a>.</li>
<li><b>BioTable-CEA-WD Task</b>: Assigning a Wikidata entity to a cell. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/biotable-cell-entity-annotation-by-wikidata-biotable-cea-wd">See AIcrowd page</a>.</li>
<li><b>BioTable-CPA-WD Task</b>: Assigning a Wikidata property to a column pair (order matters). <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/biotable-column-property-annotation-by-wikidata-biotable-cpa-wd">See AIcrowd page</a>.</li>
<li><b>AG (HardTable) Datasets and targets:</b> <a href="./data/HardTable_CTA_CEA_CPA_WD_Round2.tar.gz" target="blank">tables of HardTable-CTA-WD, HardTable-CEA-WD and HardTable-CPA-WD</a>, <a href="./data/HardTable_CTA_WD_Round2_Targets.csv" target="blank">HardTable-CTA-WD targets</a>, <a href="./data/HardTable_CEA_WD_Round2_Targets.csv" target="blank">HardTable-CEA-WD targets</a>, <a href="./data/HardTable_CPA_WD_Round2_Targets.csv" target="blank">HardTable-CPA-WD targets</a>.</li>
<li><b>HardTable-CTA-WD Task</b>: Assigning a Wikidata semantic type (a Wikidata entity as fine-grained as possible) to a column. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/hardtable-column-type-annotation-by-wikidata-hardtable-cta-wd">See AIcrowd page</a>.</li>
<li><b>HardTable-CEA-WD Task</b>: Assigning a Wikidata entity to a cell. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/hardtable-cell-entity-annotation-by-wikidata-hardtable-cea-wd">See AIcrowd page</a>.</li>
<li><b>HardTable-CPA-WD Task</b>: Assigning a Wikidata property to a column pair (order matters). <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/hardtable-column-property-annotation-by-wikidata-hardtable-cpa-wd">See AIcrowd page</a>.</li>
</ul>

<br>

<h4> Round 3: </h4>
<ul>
<li><b>Knowledge Graphs:</b> Schema.org (version: <a href="https://gittables.github.io/">May 2021</a>),  DBPedia (version: <a href="http://downloads.dbpedia.org/wiki-archive/downloads-2016-10.html">2016-10</a>), Wikidata (version: <a href="https://doi.org/10.5281/zenodo.6153449">20210828</a>)</li>
<li><b>BioDivTab Datasets and targets:</b> <a href="./data/BioDivTab_CTA_CEA_WD_Round3.tar.gz" target="blank">tables of BioDivTab-CTA-WD and BioDivTab-CEA-WD</a>, <a href="./data/BioDivTab_CTA_WD_Round3_Targets.csv" target="blank">BioDivTab-CTA-WD targets</a>, <a href="./data/BioDivTab_CEA_WD_Round3_Targets.csv" target="blank">BioDivTab-CEA-WD targets</a>. (Knowledge Graph: "Live Edition" of Wikidata)</li>
<li><b>GitTables Datasets and targets:</b> <a href="data/GitTables_CTA_DBP_SCH_Round3.tar.gz" target="blank">tables of GitTables-CTA-DBP and GitTables-CTA-SCH (one column by one Schema.org class such as schema:author and schema:URL)</a>, <a href="./data/GitTables_CTA_DBP_Round3_Targets.csv" target="blank">GitTables-CTA-DBP targets</a>, <a href="./data/GitTables_CTA_DBP_Round3_Labels.csv">GitTables-CTA-DBP labels (including DBpedia properties)</a>, <a href="data/GitTables_CTA_SCH_Round3_Targets.csv" target="blank">GitTables-CTA-SCH targets</a>, <a href="./data/GitTables_CTA_SCH_Round3_Labels.csv">GitTables-CTA-SCH labels (including properties and types from Schema.org)</a> </li>
<li><b>AG (HardTable) R3 Datasets and targets:</b> <a href="./data/HardTablesR3_CTA_CEA_CPA_WD_Round3.tar.gz" target="blank">tables of HardTablesR3-CTA-WD, HardTablesR3-CEA-WD and HardTablesR3-CPA-WD</a>, <a href="./data/HardTablesR3_CTA_WD_Round3_Targets.csv" target="blank">HardTablesR3-CTA-WD targets</a>, <a href="./data/HardTablesR3_CEA_WD_Round3_Targets.csv" target="blank">HardTablesR3-CEA-WD targets</a>, <a href="./data/HardTablesR3_CPA_WD_Round3_Targets.csv" target="blank">HardTablesR3-CPA-WD targets</a>. (Knowledge Graph: Wikidata (version: <a href="https://dumps.wikimedia.org/wikidatawiki/entities/20210823/">20210823</a>))</li>
</ul> -->

<!-- <h3>Usability Track</h3>
This new track addresses a pain point in the community regarding a lack of publicly available easy-to-use and generic solution that will address the needs of a variety of applications and settings. 
We will devise a clear scoring mechanism to rank every participant's solution in terms of several usability criteria as judged by a review panel, for example:
<ol>
<li>Is the solution open-source?</li>
<li>Does the solution require specific platform that could affect its use in common settings?</li>
<li>Does the solution require extensive training and tuning for a new application/domain?</li>
<li>Is the solution offered as a public service?</li>
<li>Does the solution include a well-designed user interface?</li>
</ol> -->


<!-- <h3>Applications Track</h3>

<p>This new track aims at addressing applications in real-world settings that take advantage of the output of the matching systems.
Challenging dataset proposals are also more than welcome.</p>


Examples include but are not limited to: 
<ol>
<li>Applications in generic data discovery and exploration.</li>
<li>Applications of table understanding for scientific corpora.</li>
<li>Applications in feature engineering and automated machine learning.</li>
</ol> -->

<!-- <p>
<b>Bio-Track:</b> Due to advances in biological research techniques, new data is constantly being produced in the biomedical domain and it is commonly published unstructured or tabular formats. This data is not trivial to integrate semantically due not only to its sheer amount but also the complexity of the biological relations between entities. Specifically, for tabular data annotation, the representation of data can have a significant impact in performance since each entity can be represented by alphanumeric codes (e.g., chemical formulas or gene names) or even have multiple synonyms. Therefore, the domain would greatly benefit from automated methods to map entities, entity types and properties to existing datasets to speed-up the process of integrating new data in the domain.
</p> -->

<br/>

<!--
<li><b>October 21:</b> System paper submissions (via <a href="https://easychair.org/conferences/?conf=semtab2022" target="blank">easychair</a>).</li>
<li><b>October 25:</b> <a href="http://om2022.ontologymatching.org/" target="blank">Ontology Matching workshop</a>.</li>
<li><b>October 23-27:</b> <a href="https://iswc2022.semanticweb.org/" target="blank">Challenge Presentation</a> and prize announcement.</li>
<li><b>November 15:</b> Final version system papers (via <a href="https://easychair.org/conferences/?conf=semtab2022" target="blank">easychair</a>).</li>
 --> 


<h2><a name="papers">Paper guidelines</a></h2>

<p>We invite participants in the Accuracy Track as well as the Datasets Track to submit a paper using easychair.<br>
System papers in the Accuracy Track should be no more than 12 pages long (excluding references) and papers for the Datasets Track are limited to 6 pages.<br>
Both type of papers should be formatted using the 
<a href="https://www.overleaf.com/latex/templates/template-for-submissions-to-ceur-workshop-proceedings-ceur-ws-dot-org/wqyfdgftmcfw" target="blank">CEUR Latex template</a> or the 
<a href="https://ceurws.wordpress.com/2020/03/31/ceurws-publishes-ceurart-paper-style/" target="blank">CEUR Word template</a>.
Papers will be reviewed by 1-2 challenge organisers.</p>
 
<p>
Accepted papers will be published as a volume of <a href="http://ceur-ws.org/" target="blank">CEUR-WS</a>. 
By submitting a paper, the authors accept the CEUR-WS publishing rules.
</p>


<br />


<h2>Organisation</h2>


<p>This challenge is organised by 
<a href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-Kavitha.Srinivas" target="blank">Kavitha Srinivas</a> (IBM Research), 
<a href="https://www.city.ac.uk/people/academics/ernesto-jimenez-ruiz" target="blank">Ernesto Jim&eacute;nez-Ruiz</a> (City, University of London; University of Oslo), 
<a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-hassanzadeh" target="blank">Oktie Hassanzadeh</a> (IBM Research), 
<a href="https://www.cs.ox.ac.uk/people/jiaoyan.chen/" target="blank">Jiaoyan Chen</a> (University of Oxford),
<a href="https://sites.google.com/site/vefthym/" target="blank">Vasilis Efthymiou</a> (FORTH - ICS),
<a href="https://vcutrona.github.io/" target="blank">Vincenzo Cutrona</a> (SUPSI), 
<a href="https://juansequeda.com/" target="blank">Juan Sequeda</a> (data.world),
<!--<a href="https://danielapoliveira.github.io/" target="blank">Daniela Oliveira</a> (Universidade de Lisboa), 
<a href="http://www.di.fc.ul.pt/~catiapesquita/" target="blank">Catia Pesquita</a> (Universidade de Lisboa),
-->
<a href="https://fusion.cs.uni-jena.de/fusion/members/nora-abdelmageed/" target="blank">Nora Abdelmageed</a> (University of Jena), and
<a href="https://madelonhulsebos.github.io/" target="blank">Madelon Hulsebos</a> (Sigma Computing, University of Amsterdam).

 
If you have any problems working with the datasets or any suggestions 
related to this challenge, do not hesitate to contact us via the <a href="https://groups.google.com/g/sem-tab-challenge">discussion group</a>.</p>


<br />

<h2>Acknowledgements</h2>


<p>
The challenge is currently supported by the <a href="http://sirius-labs.no/" target="blank">SIRIUS Centre for Research-driven Innovation</a> 
 and <a href="http://www.research.ibm.com/" target="blank">IBM Research</a>.
</p>


<p>
BiodivTab is credited to Nora Abdelmageed, Sirko Schindler, Birgitta K&ouml;nig-Ries, Heinz Nixdorf Chair for Distributed Information Systems, Friedrich Schiller University Jena, Germany.
The tables provided in this challenge are based on real biodiversity research datasets, but have been adapted for the challenge. In the form provided here, they may be used for the challenge, only. 
</p>

<p>
Any publication on challenge results needs to contain citations of the underlying datasets.  
</p>


<br />

</body></html>
