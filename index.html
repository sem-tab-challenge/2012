<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>

    <title>Semantic Web Challenge on Tabular Data to Knowledge Graph Matching</title>
    <link rel="stylesheet" type="text/css" href="style.css"/>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-0evHe/X+R7YkIZDRvuzKMRqM+OrBnVFBL6DOitfPri4tjfHxaWutUpFmBp4vmVor"
          crossorigin="anonymous"/>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.3/font/bootstrap-icons.css">
</head>
<body>
<nav id="navbar" class="navbar navbar-expand-lg bg-light sticky-top">
    <div class="container-fluid">
        <span class="navbar-brand mb-0 h1">SemTab 2022</span>
        <button class="navbar-toggler"
                type="button"
                data-bs-toggle="collapse"
                data-bs-target="#navbarSupportedContent"
                aria-controls="navbarSupportedContent"
                aria-expanded="false"
                aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                <li class="nav-item">
                    <a class="nav-link" href="#about">About</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#forum">Participate!</a>
                </li>
                <li class="nav-item dropdown">
                    <a class="nav-link dropdown-toggle"
                       href="#"
                       id="navbarDropdownTracks"
                       role="button"
                       data-bs-toggle="dropdown"
                       aria-expanded="false">
                        Tracks
                    </a>
                    <ul class="dropdown-menu" aria-labelledby="navbarDropdownTracks">
                        <li>
                            <a class="dropdown-item" href="#accuracy-track">Accuracy Track</a>
                        </li>
                        <li>
                            <a class="dropdown-item" href="#datasets-track">Datasets Track</a>
                        </li>
                        <li>
                            <a class="dropdown-item" href="#artifacts-track">Artifacts Availability Badge</a>
                        </li>
                    </ul>
                </li>
                <li class="nav-item dropdown">
                    <a class="nav-link dropdown-toggle"
                       href="#"
                       id="navbarDropdownRounds"
                       role="button"
                       data-bs-toggle="dropdown"
                       aria-expanded="false">
                        Datasets and Tasks
                    </a>
                    <ul class="dropdown-menu" aria-labelledby="navbarDropdownRounds">
                        <li>
                            <a class="dropdown-item" href="#round1">Round #1</a>
                        </li>
                        <li>
                            <a class="dropdown-item" href="#round2">Round #2</a>
                        </li>
                        <li>
                            <a class="dropdown-item" href="#round3">Round #3</a>
                        </li>
                    </ul>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#results">Results</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#paper">Paper Guidelines</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#organisation">Organisation</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#acknowledgements">Acknowledgements</a>
                </li>
            </ul>
            <span class="navbar-text">Semantic Web Challenge on Tabular Data to Knowledge Graph Matching</span>
        </div>
    </div>
</nav>
<!--<div class="alert alert-warning" role="alert">-->
<!--  <b>News (09/03/2022):</b> The <a class="alert-link" href="http://ceur-ws.org/Vol-3103/" target="_blank">SemTab 2021 proceedings</a> are out. <a href="#results" class="alert-link">Results</a> and <a href="#gt" class="alert-link">ground</a> truths are available.-->
<!--</div>-->
<div class="container-fluid">
    <div class="row">
        <div class="col-md-10 col-9"
             data-bs-spy="scroll"
             data-bs-target="#navbar"
             data-bs-root-margin="0px 0px -40%"
             data-bs-smooth-scroll="true"
             tabindex="0">
            <h2 class="title display-6" id="about">
                <i class="bi bi-trophy"></i>
                About the Challenge
            </h2>


            <p>
                Tabular data in the form of CSV files is the common input format in
                a data analytics pipeline. However, a lack of understanding of the
                semantic structure and meaning of the content may hinder the data
                analytics process. Thus gaining this semantic understanding will be
                very valuable for data integration, data cleaning, data mining,
                machine learning and knowledge discovery tasks. For example,
                understanding what the data is can help assess what sorts of
                transformation are appropriate on the data.
            </p>

            <p>
                Tables on the Web may also be the source of highly valuable data.
                The addition of semantic information to Web tables may enhance a
                wide range of applications, such as web search, question answering,
                and knowledge base (KB) construction.
            </p>

            <p>
                Tabular data to Knowledge Graph (KG) matching is the process of
                assigning semantic tags from Knowledge Graphs (e.g., Wikidata or
                DBpedia) to the elements of the table. This task however is often
                difficult in practice due to metadata (e.g., table and column names)
                being missing, incomplete or ambiguous.
            </p>

            <p>
                The <a href="http://www.cs.ox.ac.uk/isg/challenges/sem-tab/">SemTab challenge</a>
                aims at benchmarking systems dealing with the tabular data to KG
                matching problem, so as to facilitate their comparison on the same
                basis and the reproducibility of the results.
            </p>

            <p>
                The <b>2022 edition</b> of this challenge will be collocated with the
                <a href="https://iswc2022.semanticweb.org/" target="_blank">
                    21st International Semantic Web Conference
                </a>
                and the
                <a href="http://om2022.ontologymatching.org/" target="_blank"
                >17th International Workshop on Ontology Matching</a
                >.
            </p>

            <h2 class="title display-6 pt-5" id="forum">
                <i class="bi bi-chat-square-text"></i>
                Participation: Forum and Registration
            </h2>

            <p>
                We have a
                <a href="https://groups.google.com/d/forum/sem-tab-challenge" target="_blank">discussion group</a>
                for the challenge where we share the latest news with the
                participants and we discuss issues risen during the evaluation
                rounds.
            </p>

            <p>
                Please register your system using this
                <a href="https://bit.ly/semtab2022-participation" target="_blank">google form</a>.
            </p>

            <p>
                Note that participants can join SemTab at any Round for any of the
                tasks/tracks.
            </p>

            <h2 class="title display-6 pt-5" id="tracks">
                <i class="bi bi-layout-wtf"></i>
                Challenge Tracks
            </h2>

            <h3 class="pt-4" id="accuracy-track">Accuracy Track</h3>

            The evaluation of systems regarding accuracy is similar to prior
            versions of the SemTab.<br/>
            That is, to illustrate the accuracy of the submissions, we evaluate
            systems on typical multi-class classification metrics as detailed
            below.<br/>
            In addition, we adopt the "cscore" for the CTA task to reflect the
            distance in the type hierarchy between the predicted column type and
            the ground truth semantic type.<br/>
            <br/>
            <br/>
            Matching Tasks:
            <ul>
                <li>
                    <b>CTA Task</b>: Assigning a semantic type (a DBpedia class as
                    fine-grained as possible) to a column.
                </li>
                <li><b>CEA Task</b>: Matching a cell to a Wikidata entity.</li>
                <li>
                    <b>CPA Task</b>: Assigning a KG property to the relationship
                    between two columns.
                </li>
            </ul>

            Matching Criteria:
            <ul>
                <li>Average Precision</li>
                <li>Average Recall</li>
                <li>Average F1</li>
                <li>Cscore</li>
            </ul>

            Important Dates (all 2022, deadlines 11:59pm AoE):
            <ul>
                <li><b>May 26:</b> First call for challenge participants.</li>
                <li><b>June 13 - July 14:</b> Round 1.</li>
                <li><b>July 18 - September 1:</b> Round 2.</li>
                <li>
                    <b>August 15:</b> Inivations to present at the
                    <a href="https://iswc2022.semanticweb.org/" target="_blank"
                    >ISWC conference</a
                    >.
                </li>
                <li><b>September 19 - October 15:</b> Round 3.</li>
                <li>
                    <b>October 21</b>: Paper submissions (via <a
                        href="https://easychair.org/conferences/?conf=semtab2022" target="_blank">easychair</a>), and
                    artifact
                    publication.
                </li>
                <li>
                    <b>October 23 - 27</b>: Challenge presentation during OM workshop.
                </li>
                <li>
                    <b>October 23 - 27</b>: Challenge Presentation and prize
                    announcement during ISWC.
                </li>
                <li><b>November 15</b>: Final version papers (via <a
                        href="https://easychair.org/conferences/?conf=semtab2022" target="_blank">easychair</a>).
                </li>
            </ul>

            <h3 class="pt-4" id="datasets-track">Datasets Track</h3>

            The data that table-to-Knowledge-Graph matching systems are trained
            and evaluated on, is critical for their accuracy and relevance.<br/>
            We invite dataset submissions that provide challenging and accessible
            new datasets to advance the state-of-the-art of table-to-KG matching
            systems.<br/>
            Preferably, these datasets provide tables along with their ground
            truth annotations for at least one of CEA, CTA and CPA tasks.<br/>
            The dataset may be general or specific to a certain domain.<br/>
            <br/>
            Submissions will be evaluated according to provide the following:
            <ul>
                <li>
                    Description of the data collection, curation, and annotation
                    processes.
                </li>
                <li>
                    Availability of documentation with insights in the dataset
                    content.
                </li>
                <li>
                    Publicly accessible link to the dataset (e.g. Zenodo) and its DOI.
                </li>
                <li>Explanation of maintenance and long-term availability.</li>
                <li>Clear description of the envisioned use-cases.</li>
                <li>
                    Application in which the dataset is used to solve an exemplar
                    task.
                </li>
            </ul>

            We ask participants to describe their datasets submissions via
            <a href="https://easychair.org/conferences/?conf=semtab2022" target="_blank">easychair</a>
            in a short paper (max 6 pages) that discusses how the above
            criteria are covered, while also including a link to the resources.
            The link to the resources may be private, until the submission is
            evaluated by the SemTab organisers. See paper guidelines below, for
            more details. More guidance for creating, documenting and publishing
            datasets can be found
            <a
                    href="https://neurips.cc/Conferences/2022/CallForDatasetsBenchmarks"
                    target="_blank"
            >here</a
            >.

            <br/>
            <br/>
            Important Dates:
            <ul>
                <li>
                    <b>August <s>15</s> 25:</b> Paper submissions (via <a
                        href="https://easychair.org/conferences/?conf=semtab2022" target="_blank">easychair</a>), and
                    artifact
                    publication.
                </li>
                <li><b>September 30:</b> Notification of accept/reject.</li>
                <li>
                    <b>October 23 - 27</b>: Dataset Presentation and prize
                    announcement during ISWC.
                </li>
                <li><b>November 15</b>: Final version papers (via <a
                        href="https://easychair.org/conferences/?conf=semtab2022" target="_blank">easychair</a>).
                </li>
            </ul>

            <h3 class="pt-4" id="artifacts-track">Artifacts Availability Badge</h3>

            New this year is the Artifacts Availability Badge which is applicable
            to the Accuracy Track as well as the Datasets Track.<br/>
            The goal of this badge is to motivate authors to publish and document
            their systems, code, and data, so that others can use these artifacts
            and potentially reproduce or build on the results.<br/>
            This badge is given if all resources are verified to satisfy the below
            criteria.<br/>
            <br/>
            The criteria used to assess submissions (both accuracy and dataset
            submissions) are:
            <ul>
                <li>Publicly accessible data (if applicable).</li>
                <li>Publicly accessible source code.</li>
                <li>Clear documentation of the code and data.</li>
                <li>Open-source dependencies.</li>
            </ul>

            <!-- BELOW FROM 2020: -->

            <!-- <p>
        The challenge will be run with the support of the
        <a href="https://www.aicrowd.com/challenges/semtab-2021" target="_blank">AICrowd platform</a> and the <a href="https://bitbucket.org/disco_unimib/stiltool/" target="_blank">STILTool system</a>.</p>
         -->


        <h2 class="title display-6 pt-5" id="tasks">
            <i class="bi bi-table"></i>
            Datasets and tasks per round
        </h2>

        <h3 class="pt-4" id="round1">Round #1</h3>
            <ul class="nav nav-tabs mb-3" id="round1-tasks-tab" role="tablist">
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link active"
                            id="round1-cta-wd-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round1-cta-wd-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round1-cta-wd-tab-pane"
                            aria-selected="true"
                    >
                        CTA-WD (Round #1)
                    </button>
                </li>
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link"
                            id="round1-cea-wd-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round1-cea-wd-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round1-cea-wd-tab-pane"
                            aria-selected="false"
                    >
                        CEA-WD (Round #1)
                    </button>
                </li>
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link"
                            id="round1-cpa-wd-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round1-cpa-wd-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round1-cpa-wd-tab-pane"
                            aria-selected="false"
                    >
                        CPA-WD (Round #1)
                    </button>
                </li>
            </ul>
            <div class="tab-content" id="round1-tasks-tab-content">
                <div
                        class="tab-pane fade show active"
                        id="round1-cta-wd-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round1-cta-wd-tab"
                        tabindex="0"
                >
                    <h4>Column Type Annotation by Wikidata (CTA-WD)</h4>
                    <p>
                        This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                        Matching".
                        It's to annotate an entity column (i.e., a column composed of entity mentions) in a table with
                        types from <strong>Wikidata</strong>
                        (version: <a href="https://zenodo.org/record/6643443">20220521</a>) <br>
                        <i class="bi bi-lightbulb-fill" style="color: var(--bs-yellow)"></i>
                        Notes: participants may use the public Wikidata endpoint (or its API) since the above dump is
                        very recent.
                    </p>
                    <h5>Task Description</h5>
                    <p>
                        The task is to annotate each entity column by items of Wikidata as its type.
                        Each column can be annotated by multiple types:
                        the one that is as fine grained as possible and correct to all the column cells, is regarded as
                        a <strong>perfect annotation</strong>;
                        the one that is the ancestor of the perfect annotation is regarded as an <strong>okay
                        annotation</strong>;
                        others are regarded as <strong>wrong annotations</strong>.
                    </p>
                    <p>
                        The annotation can be a normal entity of Wikidata, with the prefix of
                        http://www.wikidata.org/entity/, such as http://www.wikidata.org/entity/Q8425. Each column
                        should be annotated by <strong>at most one item</strong>. A perfect annotation is encouraged
                        with a full score,
                        while an okay annotation can still get a part of the score. Example:
                        "KIN0LD6C","0","http://www.wikidata.org/entity/Q8425". Please use the prefix of
                        http://www.wikidata.org/entity/ instead of the URL prefix https://www.wikidata.org/wiki/.
                    </p>
                    <p>
                        The annotation should be represented by its full IRI, where the case is NOT sensitive. Each
                        submission should be a CSV file. Each line should include a column identified by table id and
                        column id, and the column's annotation (a Wikidata item). It means one line should include three
                        fields: "Table ID", "Column ID" and "Annotation IRI". The headers should be excluded from the
                        submission file.
                    </p>

                    Notes:
                    <ol>
                        <li>
                            Table ID is the filename of the table data, but does NOT include the extension.
                        </li>
                        <li>
                            Column ID is the position of the column in the input, starting from 0, i.e., first column's
                            ID is 0.
                        </li>
                        <li>
                            One submission file should have NO duplicate lines for each target column.
                        </li>
                        <li>
                            Annotations for columns out of the target columns are ignored.
                        </li>
                    </ol>


                    <h5>Dataset</h5>
                    <dl class="row">
                        <dt class="col-sm-3">Link</dt>
                        <dd class="col-sm-9"><a
                                href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/HardTablesR1.tar.gz"
                                target="_blank">Round #1 HardTables Dataset</a></dd>

                        <dt class="col-sm-3">Description</dt>
                        <dd class="col-sm-9">
                            <p>The dataset contains:</p>
                            <ul>
                                <li>evaluator codes (CTA_WD_Evaluator.py)</li>
                                <li>the validation set (DataSets/HardTablesR1/Valid/gt/cta_gt.csv,
                                    DataSets/HardTablesR1/Valid/gt/cta_gt_ancestor.json,
                                    DataSets/HardTablesR1/Valid/gt/cta_gt_descendent.json,
                                    DataSets/HardTablesR1/Valid/tables)
                                </li>
                                <li>the testing set (DataSets/HardTablesR1/Test/tables,
                                    DataSets/HardTablesR1/Test/target/cta_gt.csv)
                                </li>
                            </ul>
                        </dd>

                        <dt class="col-sm-3">Format</dt>
                        <dd class="col-sm-9">
                            One table is stored in one CSV file. Each line corresponds to a table row. The
                            first row may either be the table header or content. The target columns for annotation are
                            saved
                            in a CSV file. The CTA GTs' ancestors and descendents are saved in two json files,
                            respectively.
                        </dd>
                    </dl>

                    <h5>Evaluation Criteria</h5>
                    <p>
                        We encourage one perfect annotation, and at same time score one of its ancestors (okay
                        annotation). Thus we calculate Approximate Precision (\(APrecision\)), Approximate Recall
                        (\(ARecall\)), and Approximate F1 Score (\(AF1\)):

                        \[APrecision = {\sum_{a \in all\ annotations}g(a) \over all\ annotations\ \#}\]

                        \[ARecall = {\sum_{col \in all\ target\ columns}(max\_annotation\_score(col)) \over all\ target\
                        columns\ \#}\]

                        \[AF1 = {2 \times APrecision \times ARecall \over APrecision + ARecall}\]

                    </p>

                    Notes:

                    <ol>
                        <li>
                            # denotes the number.
                        </li>
                        <li>
                            \(
                            g(a) =
                            \begin{cases}
                            1.0, & \text{ if } a \text{ is a perfect annotation} \\
                            0.8^{d(a)}, & \text{ if } a \text{ is an ancestor of the perfect annotation and } d(a) < 5
                            \\
                            0.7^{d(a)}, & \text{ if } a \text{ is a descendent of the perfect annotation and } d(a) < 3
                            \\
                            0, & otherwise
                            \end{cases}
                            \)
                            <p>
                                where \(d(a)\) is the depth to the perfect annotation.
                                E.g., \(d(a)=1\) if \(a\) is a parent of the perfect annotation, and \(d(a)=2\) if \(a\)
                                is a grandparent of the perfect annotation.
                            </p>
                        </li>
                        <li>
                            \(
                            max\_annotation\_score(col) =
                            \begin{cases}
                            g(a), & \text{ if } col \text{ has an annotation } a \\
                            0, & \text{ if } col \text{ has no annotation }
                            \end{cases}

                            \)
                        </li>
                        <li>
                            \(AF1\) is used as the primary score, and \(APrecision\) is used as the secondary score.
                        </li>
                        <li>
                            A cell may have multiple equivalent Wikidata items as its GT (e.g., redirected pages
                            Q20514736 and Q852446). For an annotated entity, our evaluator will calculate the score with
                            each GT entity and select the maximum score.
                        </li>
                    </ol>

                    <h5>Submission</h5>
                    Participants can test and develop their systems on the given ground truth (validation set).
                    They can weekly <a href="https://bit.ly/semtab2022-round1" target="_blank"> upload </a> their
                    annotations corresponding to the
                    targets (test set).
                    <!--https://forms.gle/nrkQqNje6PrpuPGU9-->
                </div>
                <div
                        class="tab-pane fade"
                        id="round1-cea-wd-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round1-cea-wd-tab"
                        tabindex="0"
                >
                    <h4>Cell Entity Annotation by Wikidata (CEA-WD)</h4>
                    <p>
                        This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                        Matching".
                        It is to annotate column cells (entity mentions) in a table with entities of
                        <strong>Wikidata</strong>
                        (version: <a href="https://zenodo.org/record/6643443">20220521</a>) <br>
                        <i class="bi bi-lightbulb-fill" style="color: var(--bs-yellow)"></i>
                        Notes: participants may use the public Wikidata endpoint (or its API) since the above dump is
                        very recent.
                    </p>
                    <h5>Task Description</h5>
                    <p>
                        The task is to annotate each target cell with an entity of Wikidata.
                        Each submission should contain the annotation of the target cell. One cell can be annotated by
                        one entity with the prefix of http://www.wikidata.org/entity/. Any of the equivalent entities of
                        the ground truth entity are regarded as correct. Case is NOT sensitive.
                    </p>
                    <p>
                        The submission file should be in CSV format.
                        Each line should contain the annotation of one cell which is identified by a table id, a column
                        id and a row id.
                        Namely one line should have four fields: "Table ID", "Row ID", "Column ID" and "Entity IRI".
                        Each cell should be annotated by <strong>at most one entity</strong>.
                        The headers should be excluded from the submission file.
                        Here is an example: "OHGI1JNY","32","1","http://www.wikidata.org/entity/Q5484".
                        Please use the prefix of http://www.wikidata.org/entity/ instead of
                        https://www.wikidata.org/wiki/ which is the prefix of the Wikidata page URL.
                    </p>

                    Notes:
                    <ol>
                        <li>
                            Table ID does not include filename extension; make sure you remove the .csv extension from
                            the filename.
                        </li>
                        <li>
                            Column ID is the position of the column in the table file, starting from 0, i.e., first
                            column's ID is 0.
                        </li>
                        <li>
                            Row ID is the position of the row in the table file, starting from 0, i.e., first row's ID
                            is 0.
                        </li>
                        <li>
                            One submission file should have NO duplicate lines for one cell.
                        </li>
                        <li>
                            Annotations for cells out of the target cells are ignored.
                        </li>
                    </ol>


                    <h5>Dataset</h5>
                    <dl class="row">
                        <dt class="col-sm-3">Link</dt>
                        <dd class="col-sm-9"><a
                                href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/HardTablesR1.tar.gz"
                                target="_blank">Round #1 HardTables Dataset</a></dd>

                        <dt class="col-sm-3">Description</dt>
                        <dd class="col-sm-9">
                            <p>The dataset contains:</p>
                            <ul>
                                <li>evaluator codes (CEA_WD_Evaluator.py)</li>
                                <li>the validation set (DataSets/HardTablesR1/Valid/gt/cea_gt.csv,
                                    DataSets/HardTablesR1/Valid/tables)
                                </li>
                                <li>the testing set (DataSets/HardTablesR1/Test/tables,
                                    DataSets/HardTablesR1/Test/target/cea_target.csv)
                                </li>
                            </ul>
                        </dd>

                        <dt class="col-sm-3">Format</dt>
                        <dd class="col-sm-9">
                            One table is stored in one CSV file. Each line corresponds to a table row. The first row may
                            either be the table header or content. The target cells for annotation are saved in a CSV
                            file.
                        </dd>
                    </dl>

                    <h5>Evaluation Criteria</h5>
                    <p>
                        Precision, Recall and F1 Score are calculated:

                        \[Precision = {{correct\_annotations \#} \over {submitted\_annotations \#}}\]

                        \[Recall = {{correct\_annotations \#} \over {ground\_truth\_annotations \#}}\]

                        \[F1 = {2 \times Precision \times Recall \over Precision + Recall}\]

                    </p>

                    Notes:

                    <ol>
                        <li>
                            # denotes the number.
                        </li>
                        <li>
                            \(F1\) is used as the primary score, and \(Precision\) is used as the secondary score.
                        </li>
                        <li>
                            One target cell, one ground truth annotation, i.e., # ground truth annotations = # target
                            cells. The ground truth annotation has already covered all equivalent entities (e.g., wiki
                            page redirected entities); the ground truth is hit if one of its equivalent entities is hit.
                        </li>
                    </ol>

                    <h5>Submission</h5>
                    Participants can test and develop their systems on the given ground truth (validation set).
                    They can weekly <a href="https://bit.ly/semtab2022-round1" target="_blank"> upload </a> their annotations corresponding to the
                    targets (test set).
                </div>
                <div
                        class="tab-pane fade"
                        id="round1-cpa-wd-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round1-cpa-wd-tab"
                        tabindex="0"
                >
                    <h4>Column Property Annotation by Wikidata (CPA-WD)</h4>
                    <p>
                        This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                        Matching".
                        It is to annotate column relationships in a table with properties of
                        <strong>Wikidata</strong>
                        (version: <a href="https://zenodo.org/record/6643443">20220521</a>) <br>
                        <i class="bi bi-lightbulb-fill" style="color: var(--bs-yellow)"></i>
                        Notes: participants may use the public Wikidata endpoint (or its API) since the above dump is
                        very recent.
                    </p>
                    <h5>Task Description</h5>
                    <p>
                        The task is to annotate each column pair with a property of Wikidata.
                        Each submission should contain an annotation of a target column pair. Note the order of the two
                        columns matters. The annotation property should start with the prefix of
                        http://www.wikidata.org/prop/direct/. Case is NOT sensitive.
                    </p>
                    <p>
                        The submission file should be in CSV format.
                        Each line should contain the annotation of two columns which is identified by a table id, column
                        id one and column id two.
                        Namely one line should have four fields: "Table ID", "Column ID 1", "Column ID 2" and "Property
                        IRI".
                        Each column pair should be annotated by <strong>at most one property</strong>.
                        The headers should be excluded from the submission file.
                        Here is an example: "OHGI1JNY","0","1","http://www.wikidata.org/prop/direct/P702".
                        Please use the prefix of http://www.wikidata.org/prop/direct/ instead of
                        https://www.wikidata.org/wiki/ which is the prefix of the Wikidata page URL.
                    </p>

                    Notes:
                    <ol>
                        <li>
                            Table ID does not include filename extension; make sure you remove the .csv extension from
                            the filename.
                        </li>
                        <li>
                            Column ID is the position of the column in the table file, starting from 0, i.e., first
                            column's ID is 0.
                        </li>
                        <li>
                            One submission file should have NO duplicate lines for one column pair.
                        <li>
                            Annotations for column pairs out of the targets are ignored.
                        </li>
                    </ol>


                    <h5>Dataset</h5>
                    <dl class="row">
                        <dt class="col-sm-3">Link</dt>
                        <dd class="col-sm-9"><a
                                href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/HardTablesR1.tar.gz"
                                target="_blank">Round #1 HardTables Dataset</a></dd>

                        <dt class="col-sm-3">Description</dt>
                        <dd class="col-sm-9">
                            <p>The dataset contains:</p>
                            <ul>
                                <li>evaluator codes (CPA_WD_Evaluator.py))</li>
                                <li>the validation set (DataSets/HardTablesR1/Valid/gt/cpa_gt.csv,
                                    DataSets/HardTablesR1/Valid/tables)
                                </li>
                                <li>the testing set (DataSets/HardTablesR1/Test/tables,
                                    DataSets/HardTablesR1/Test/target/cpa_target.csv)
                                </li>
                            </ul>
                        </dd>

                        <dt class="col-sm-3">Format</dt>
                        <dd class="col-sm-9">
                            One table is stored in one CSV file. Each line corresponds to a table row. The first row may
                            either be the table header or content. The target cells for annotation are saved in a CSV
                            file.
                        </dd>
                    </dl>

                    <h5>Evaluation Criteria</h5>
                    <p>
                        Precision, Recall and F1 Score are calculated:

                        \[Precision = {{correct\_annotations \#} \over {submitted\_annotations \#}}\]

                        \[Recall = {{correct\_annotations \#} \over {ground\_truth\_annotations \#}}\]

                        \[F1 = {2 \times Precision \times Recall \over Precision + Recall}\]

                    </p>

                    Notes:

                    <ol>
                        <li>
                            # denotes the number.
                        </li>
                        <li>
                            \(F1\) is used as the primary score, and \(Precision\) is used as the secondary score.
                        </li>
                        <li>
                            One target column pair, one ground truth annotation, i.e., # ground truth annotations = #
                            target column pairs.
                        </li>
                    </ol>

                    <h5>Submission</h5>
                    Participants can test and develop their systems on the given ground truth (validation set).
                    They can weekly <a href="https://bit.ly/semtab2022-round1" target="_blank"> upload </a> their annotations corresponding to the
                    targets (test set).


                </div>
            </div>

            <h3 class="pt-4" id="round2">Round #2</h3>
            <ul class="nav nav-tabs mb-3" id="round2-tasks-tab" role="tablist">
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link active"
                            id="round2-ht-cta-wd-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round2-ht-cta-wd-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round2-ht-cta-wd-tab-pane"
                            aria-selected="true"
                    >
                        HT-CTA-WD (Round #2)
                    </button>
                </li>
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link"
                            id="round2-ht-cea-wd-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round2-ht-cea-wd-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round2-ht-cea-wd-tab-pane"
                            aria-selected="false"
                    >
                        HT-CEA-WD (Round #2)
                    </button>
                </li>
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link"
                            id="round2-ht-cpa-wd-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round2-ht-cpa-wd-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round2-ht-cpa-wd-tab-pane"
                            aria-selected="false"
                    >
                        HT-CPA-WD (Round #2)
                    </button>
                </li>
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link"
                            id="round2-2t-cta-wd-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round2-2t-cta-wd-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round2-2t-cta-wd-tab-pane"
                            aria-selected="true"
                    >
                        2T-CTA-WD (Round #2)
                    </button>
                </li>
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link"
                            id="round2-2t-cea-wd-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round2-2t-cea-wd-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round2-2t-cea-wd-tab-pane"
                            aria-selected="false"
                    >
                        2T-CEA-WD (Round #2)
                    </button>
                </li>
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link"
                            id="round2-2t-cta-dbp-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round2-2t-cta-dbp-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round2-2t-cta-dbp-tab-pane"
                            aria-selected="true"
                    >
                        2T-CTA-DBP (Round #2)
                    </button>
                </li>
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link"
                            id="round2-2t-cea-dbp-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round2-2t-cea-dbp-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round2-2t-cea-dbp-tab-pane"
                            aria-selected="false"
                    >
                        2T-CEA-DBP (Round #2)
                    </button>
                </li>
            </ul>
            <div class="tab-content" id="round2-tasks-tab-content">
                <div
                        class="tab-pane fade show active"
                        id="round2-ht-cta-wd-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round2-ht-cta-wd-tab"
                        tabindex="0"
                >
                    <h4>Column Type Annotation by Wikidata (HT-CTA-WD)</h4>
                    <p>
                        This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                        Matching".
                        It's to annotate an entity column (i.e., a column composed of entity mentions) in a table with
                        types from <strong>Wikidata</strong>
                        (version: <a href="https://zenodo.org/record/6643443">20220521</a>) <br>
                        <i class="bi bi-lightbulb-fill" style="color: var(--bs-yellow)"></i>
                        Notes: participants may use the public Wikidata endpoint (or its API) since the above dump is
                        very recent.
                    </p>
                    <h5>Task Description</h5>
                    <p>
                        The task is to annotate each entity column by items of Wikidata as its type.
                        Each column can be annotated by multiple types:
                        the one that is as fine grained as possible and correct to all the column cells, is regarded as
                        a <strong>perfect annotation</strong>;
                        the one that is the ancestor of the perfect annotation is regarded as an <strong>okay
                        annotation</strong>;
                        others are regarded as <strong>wrong annotations</strong>.
                    </p>
                    <p>
                        The annotation can be a normal entity of Wikidata, with the prefix of
                        http://www.wikidata.org/entity/, such as http://www.wikidata.org/entity/Q8425. Each column
                        should be annotated by <strong>at most one item</strong>. A perfect annotation is encouraged
                        with a full score,
                        while an okay annotation can still get a part of the score. Example:
                        "KIN0LD6C","0","http://www.wikidata.org/entity/Q8425". Please use the prefix of
                        http://www.wikidata.org/entity/ instead of the URL prefix https://www.wikidata.org/wiki/.
                    </p>
                    <p>
                        The annotation should be represented by its full IRI, where the case is NOT sensitive. Each
                        submission should be a CSV file. Each line should include a column identified by table id and
                        column id, and the column's annotation (a Wikidata item). It means one line should include three
                        fields: "Table ID", "Column ID" and "Annotation IRI". The headers should be excluded from the
                        submission file.
                    </p>

                    Notes:
                    <ol>
                        <li>
                            Table ID is the filename of the table data, but does NOT include the extension.
                        </li>
                        <li>
                            Column ID is the position of the column in the input, starting from 0, i.e., first column's
                            ID is 0.
                        </li>
                        <li>
                            One submission file should have NO duplicate lines for each target column.
                        </li>
                        <li>
                            Annotations for columns out of the target columns are ignored.
                        </li>
                    </ol>


                    <h5>Dataset</h5>
                    <dl class="row">
                        <dt class="col-sm-3">Link</dt>
                        <dd class="col-sm-9"><a
                                href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/HardTablesR2.tar.gz"
                                target="_blank">Round #2 HardTables Dataset</a></dd>

                        <dt class="col-sm-3">Description</dt>
                        <dd class="col-sm-9">
                            <p>The dataset contains:</p>
                            <ul>
                                <li>evaluator codes (CTA_WD_Evaluator.py)</li>
                                <li>the validation set (DataSets/HardTablesR2/Valid/gt/cta_gt.csv,
                                    DataSets/HardTablesR2/Valid/gt/cta_gt_ancestor.json,
                                    DataSets/HardTablesR2/Valid/gt/cta_gt_descendent.json,
                                    DataSets/HardTablesR2/Valid/tables)
                                </li>
                                <li>the testing set (DataSets/HardTablesR2/Test/tables,
                                    DataSets/HardTablesR2/Test/target/cta_gt.csv)
                                </li>
                            </ul>
                        </dd>

                        <dt class="col-sm-3">Format</dt>
                        <dd class="col-sm-9">
                            One table is stored in one CSV file. Each line corresponds to a table row. The
                            first row may either be the table header or content. The target columns for annotation are
                            saved
                            in a CSV file. The CTA GTs' ancestors and descendents are saved in two json files,
                            respectively.
                        </dd>
                    </dl>

                    <h5>Evaluation Criteria</h5>
                    <p>
                        We encourage one perfect annotation, and at same time score one of its ancestors (okay
                        annotation). Thus we calculate Approximate Precision (\(APrecision\)), Approximate Recall
                        (\(ARecall\)), and Approximate F1 Score (\(AF1\)):

                        \[APrecision = {\sum_{a \in all\ annotations}g(a) \over all\ annotations\ \#}\]

                        \[ARecall = {\sum_{col \in all\ target\ columns}(max\_annotation\_score(col)) \over all\ target\
                        columns\ \#}\]

                        \[AF1 = {2 \times APrecision \times ARecall \over APrecision + ARecall}\]

                    </p>

                    Notes:

                    <ol>
                        <li>
                            # denotes the number.
                        </li>
                        <li>
                            \(
                            g(a) =
                            \begin{cases}
                            1.0, & \text{ if } a \text{ is a perfect annotation} \\
                            0.8^{d(a)}, & \text{ if } a \text{ is an ancestor of the perfect annotation and } d(a) < 5
                            \\
                            0.7^{d(a)}, & \text{ if } a \text{ is a descendent of the perfect annotation and } d(a) < 3
                            \\
                            0, & otherwise
                            \end{cases}
                            \)
                            <p>
                                where \(d(a)\) is the depth to the perfect annotation.
                                E.g., \(d(a)=1\) if \(a\) is a parent of the perfect annotation, and \(d(a)=2\) if \(a\)
                                is a grandparent of the perfect annotation.
                            </p>
                        </li>
                        <li>
                            \(
                            max\_annotation\_score(col) =
                            \begin{cases}
                            g(a), & \text{ if } col \text{ has an annotation } a \\
                            0, & \text{ if } col \text{ has no annotation }
                            \end{cases}

                            \)
                        </li>
                        <li>
                            \(AF1\) is used as the primary score, and \(APrecision\) is used as the secondary score.
                        </li>
                        <li>
                            A cell may have multiple equivalent Wikidata items as its GT (e.g., redirected pages
                            Q20514736 and Q852446). For an annotated entity, our evaluator will calculate the score with
                            each GT entity and select the maximum score.
                        </li>
                    </ol>

                    <h5>Submission</h5>
                    Participants can test and develop their systems on the given ground truth (validation set).
                    They can weekly <a href="https://bit.ly/semtab2022-round2" target="_blank"> upload </a> their
                    annotations corresponding to the
                    targets (test set).
                </div>
                <div
                        class="tab-pane fade"
                        id="round2-ht-cea-wd-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round2-ht-cea-wd-tab"
                        tabindex="0"
                >
                    <h4>Cell Entity Annotation by Wikidata (HT-CEA-WD)</h4>
                    <p>
                        This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                        Matching".
                        It is to annotate column cells (entity mentions) in a table with entities of
                        <strong>Wikidata</strong>
                        (version: <a href="https://zenodo.org/record/6643443">20220521</a>) <br>
                        <i class="bi bi-lightbulb-fill" style="color: var(--bs-yellow)"></i>
                        Notes: participants may use the public Wikidata endpoint (or its API) since the above dump is
                        very recent.
                    </p>
                    <h5>Task Description</h5>
                    <p>
                        The task is to annotate each target cell with an entity of Wikidata.
                        Each submission should contain the annotation of the target cell. One cell can be annotated by
                        one entity with the prefix of http://www.wikidata.org/entity/. Any of the equivalent entities of
                        the ground truth entity are regarded as correct. Case is NOT sensitive.
                    </p>
                    <p>
                        The submission file should be in CSV format.
                        Each line should contain the annotation of one cell which is identified by a table id, a column
                        id and a row id.
                        Namely one line should have four fields: "Table ID", "Row ID", "Column ID" and "Entity IRI".
                        Each cell should be annotated by <strong>at most one entity</strong>.
                        The headers should be excluded from the submission file.
                        Here is an example: "OHGI1JNY","32","1","http://www.wikidata.org/entity/Q5484".
                        Please use the prefix of http://www.wikidata.org/entity/ instead of
                        https://www.wikidata.org/wiki/ which is the prefix of the Wikidata page URL.
                    </p>

                    Notes:
                    <ol>
                        <li>
                            Table ID does not include filename extension; make sure you remove the .csv extension from
                            the filename.
                        </li>
                        <li>
                            Column ID is the position of the column in the table file, starting from 0, i.e., first
                            column's ID is 0.
                        </li>
                        <li>
                            Row ID is the position of the row in the table file, starting from 0, i.e., first row's ID
                            is 0.
                        </li>
                        <li>
                            One submission file should have NO duplicate lines for one cell.
                        </li>
                        <li>
                            Annotations for cells out of the target cells are ignored.
                        </li>
                    </ol>


                    <h5>Dataset</h5>
                    <dl class="row">
                        <dt class="col-sm-3">Link</dt>
                        <dd class="col-sm-9"><a
                                href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/HardTablesR2.tar.gz"
                                target="_blank">Round #2 HardTables Dataset</a></dd>

                        <dt class="col-sm-3">Description</dt>
                        <dd class="col-sm-9">
                            <p>The dataset contains:</p>
                            <ul>
                                <li>evaluator codes (CEA_WD_Evaluator.py)</li>
                                <li>the validation set (DataSets/HardTablesR2/Valid/gt/cea_gt.csv,
                                    DataSets/HardTablesR2/Valid/tables)
                                </li>
                                <li>the testing set (DataSets/HardTablesR2/Test/tables,
                                    DataSets/HardTablesR2/Test/target/cea_target.csv)
                                </li>
                            </ul>
                        </dd>

                        <dt class="col-sm-3">Format</dt>
                        <dd class="col-sm-9">
                            One table is stored in one CSV file. Each line corresponds to a table row. The first row may
                            either be the table header or content. The target cells for annotation are saved in a CSV
                            file.
                        </dd>
                    </dl>

                    <h5>Evaluation Criteria</h5>
                    <p>
                        Precision, Recall and F1 Score are calculated:

                        \[Precision = {{correct\_annotations \#} \over {submitted\_annotations \#}}\]

                        \[Recall = {{correct\_annotations \#} \over {ground\_truth\_annotations \#}}\]

                        \[F1 = {2 \times Precision \times Recall \over Precision + Recall}\]

                    </p>

                    Notes:

                    <ol>
                        <li>
                            # denotes the number.
                        </li>
                        <li>
                            \(F1\) is used as the primary score, and \(Precision\) is used as the secondary score.
                        </li>
                        <li>
                            One target cell, one ground truth annotation, i.e., # ground truth annotations = # target
                            cells. The ground truth annotation has already covered all equivalent entities (e.g., wiki
                            page redirected entities); the ground truth is hit if one of its equivalent entities is hit.
                        </li>
                    </ol>

                    <h5>Submission</h5>
                    Participants can test and develop their systems on the given ground truth (validation set).
                    They can weekly <a href="https://bit.ly/semtab2022-round2" target="_blank"> upload </a> their annotations corresponding to the
                    targets (test set).
                </div>
                <div
                        class="tab-pane fade"
                        id="round2-ht-cpa-wd-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round2-ht-cpa-wd-tab"
                        tabindex="0"
                >
                    <h4>Column Property Annotation by Wikidata (HT-CPA-WD)</h4>
                    <p>
                        This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                        Matching".
                        It is to annotate column relationships in a table with properties of
                        <strong>Wikidata</strong>
                        (version: <a href="https://zenodo.org/record/6643443">20220521</a>) <br>
                        <i class="bi bi-lightbulb-fill" style="color: var(--bs-yellow)"></i>
                        Notes: participants may use the public Wikidata endpoint (or its API) since the above dump is
                        very recent.
                    </p>
                    <h5>Task Description</h5>
                    <p>
                        The task is to annotate each column pair with a property of Wikidata.
                        Each submission should contain an annotation of a target column pair. Note the order of the two
                        columns matters. The annotation property should start with the prefix of
                        http://www.wikidata.org/prop/direct/. Case is NOT sensitive.
                    </p>
                    <p>
                        The submission file should be in CSV format.
                        Each line should contain the annotation of two columns which is identified by a table id, column
                        id one and column id two.
                        Namely one line should have four fields: "Table ID", "Column ID 1", "Column ID 2" and "Property
                        IRI".
                        Each column pair should be annotated by <strong>at most one property</strong>.
                        The headers should be excluded from the submission file.
                        Here is an example: "OHGI1JNY","0","1","http://www.wikidata.org/prop/direct/P702".
                        Please use the prefix of http://www.wikidata.org/prop/direct/ instead of
                        https://www.wikidata.org/wiki/ which is the prefix of the Wikidata page URL.
                    </p>

                    Notes:
                    <ol>
                        <li>
                            Table ID does not include filename extension; make sure you remove the .csv extension from
                            the filename.
                        </li>
                        <li>
                            Column ID is the position of the column in the table file, starting from 0, i.e., first
                            column's ID is 0.
                        </li>
                        <li>
                            One submission file should have NO duplicate lines for one column pair.
                        <li>
                            Annotations for column pairs out of the targets are ignored.
                        </li>
                    </ol>


                    <h5>Dataset</h5>
                    <dl class="row">
                        <dt class="col-sm-3">Link</dt>
                        <dd class="col-sm-9"><a
                                href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/HardTablesR2.tar.gz"
                                target="_blank">Round #2 HardTables Dataset</a></dd>

                        <dt class="col-sm-3">Description</dt>
                        <dd class="col-sm-9">
                            <p>The dataset contains:</p>
                            <ul>
                                <li>evaluator codes (CPA_WD_Evaluator.py))</li>
                                <li>the validation set (DataSets/HardTablesR2/Valid/gt/cpa_gt.csv,
                                    DataSets/HardTablesR2/Valid/tables)
                                </li>
                                <li>the testing set (DataSets/HardTablesR2/Test/tables,
                                    DataSets/HardTablesR2/Test/target/cpa_target.csv)
                                </li>
                            </ul>
                        </dd>

                        <dt class="col-sm-3">Format</dt>
                        <dd class="col-sm-9">
                            One table is stored in one CSV file. Each line corresponds to a table row. The first row may
                            either be the table header or content. The target cells for annotation are saved in a CSV
                            file.
                        </dd>
                    </dl>

                    <h5>Evaluation Criteria</h5>
                    <p>
                        Precision, Recall and F1 Score are calculated:

                        \[Precision = {{correct\_annotations \#} \over {submitted\_annotations \#}}\]

                        \[Recall = {{correct\_annotations \#} \over {ground\_truth\_annotations \#}}\]

                        \[F1 = {2 \times Precision \times Recall \over Precision + Recall}\]

                    </p>

                    Notes:

                    <ol>
                        <li>
                            # denotes the number.
                        </li>
                        <li>
                            \(F1\) is used as the primary score, and \(Precision\) is used as the secondary score.
                        </li>
                        <li>
                            One target column pair, one ground truth annotation, i.e., # ground truth annotations = #
                            target column pairs.
                        </li>
                    </ol>

                    <h5>Submission</h5>
                    Participants can test and develop their systems on the given ground truth (validation set).
                    They can weekly <a href="https://bit.ly/semtab2022-round2" target="_blank"> upload </a> their annotations corresponding to the
                    targets (test set).

                </div>
                <div
                        class="tab-pane fade"
                        id="round2-2t-cta-wd-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round2-2t-cta-wd-tab"
                        tabindex="0"
                >
                    <h4>Column Type Annotation by Wikidata (2T-CTA-WD)</h4>
                    <p>
                        This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                        Matching".
                        It's to annotate an entity column (i.e., a column composed of entity mentions) in a table with
                        types from <strong>Wikidata</strong>
                        (version: <a href="https://zenodo.org/record/6643443">20220521</a>) <br>
                        <i class="bi bi-lightbulb-fill" style="color: var(--bs-yellow)"></i>
                        Notes: participants may use the public Wikidata endpoint (or its API) since the above dump is
                        very recent.
                    </p>
                    <h5>Task Description</h5>
                    <p>
                        The task is to annotate each entity column by items of Wikidata as its type.
                        Each column can be annotated by multiple types:
                        the one that is as fine grained as possible and correct to all the column cells, is regarded as
                        a <strong>perfect annotation</strong>;
                        the one that is the ancestor of the perfect annotation is regarded as an <strong>okay
                        annotation</strong>;
                        others are regarded as <strong>wrong annotations</strong>.
                    </p>
                    <p>
                        The annotation can be a normal entity of Wikidata, with the prefix of
                        http://www.wikidata.org/entity/, such as http://www.wikidata.org/entity/Q8425. Each column
                        should be annotated by <strong>at most one item</strong>.
                    </p>
                    <div class="alert alert-warning" role="alert">
                        Use the '<strong>NIL</strong>' annotation when the column should be annotated with a concept
                        that is <strong>NOT</strong> represented in the KG.
                    </div>
                    <p>
                        A perfect annotation is encouraged with a full score,
                        while an okay annotation can still get a part of the score. Example:
                        "KIN0LD6C","0","http://www.wikidata.org/entity/Q8425". Please use the prefix of
                        http://www.wikidata.org/entity/ instead of the URL prefix https://www.wikidata.org/wiki/.
                    </p>
                    <p>
                        The annotation should be represented by its full IRI, where the case is NOT sensitive. Each
                        submission should be a CSV file. Each line should include a column identified by table id and
                        column id, and the column's annotation (a Wikidata item). It means one line should include three
                        fields: "Table ID", "Column ID" and "Annotation IRI". The headers should be excluded from the
                        submission file.
                    </p>

                    Notes:
                    <ol>
                        <li>
                            Table ID is the filename of the table data, but does NOT include the extension.
                        </li>
                        <li>
                            Column ID is the position of the column in the input, starting from 0, i.e., first column's
                            ID is 0.
                        </li>
                        <li>
                            One submission file should have NO duplicate lines for each target column.
                        </li>
                        <li>
                            Annotations for columns out of the target columns are ignored.
                        </li>
                    </ol>


                    <h5>Dataset</h5>
                    <dl class="row">
                        <dt class="col-sm-3">Link</dt>
                        <dd class="col-sm-9"><a
                                href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/ToughTablesR2-WD.tar.gz"
                                target="_blank">Round #2 ToughTables (WD) Dataset</a></dd>

                        <dt class="col-sm-3">Description</dt>
                        <dd class="col-sm-9">
                            <p>The dataset contains:</p>
                            <ul>
                                <li>evaluator codes (CTA_WD_Evaluator.py)</li>
                                <li>the validation set (DataSets/ToughTablesR2-WD/Valid/gt/cta_gt.csv,
                                    DataSets/ToughTablesR2-WD/Valid/gt/cta_gt_ancestor.json,
                                    DataSets/ToughTablesR2-WD/Valid/gt/cta_gt_descendent.json,
                                    DataSets/ToughTablesR2-WD/Valid/tables)
                                </li>
                                <li>the testing set (DataSets/ToughTablesR2-WD/Test/tables,
                                    DataSets/ToughTablesR2-WD/Test/target/cta_gt.csv)
                                </li>
                            </ul>
                        </dd>

                        <dt class="col-sm-3">Format</dt>
                        <dd class="col-sm-9">
                            One table is stored in one CSV file. Each line corresponds to a table row. The
                            first row may either be the table header or content. The target columns for annotation are
                            saved
                            in a CSV file. The CTA GTs' ancestors and descendents are saved in two json files,
                            respectively.
                        </dd>
                    </dl>

                    <h5>Evaluation Criteria</h5>
                    <p>
                        We encourage one perfect annotation, and at same time score one of its ancestors (okay
                        annotation). Thus we calculate Approximate Precision (\(APrecision\)), Approximate Recall
                        (\(ARecall\)), and Approximate F1 Score (\(AF1\)):

                        \[APrecision = {\sum_{a \in all\ annotations}g(a) \over all\ annotations\ \#}\]

                        \[ARecall = {\sum_{col \in all\ target\ columns}(max\_annotation\_score(col)) \over all\ target\
                        columns\ \#}\]

                        \[AF1 = {2 \times APrecision \times ARecall \over APrecision + ARecall}\]

                    </p>

                    Notes:

                    <ol>
                        <li>
                            # denotes the number.
                        </li>
                        <li>
                            \(
                            g(a) =
                            \begin{cases}
                            1.0, & \text{ if } a \text{ is a perfect annotation} \\
                            0.8^{d(a)}, & \text{ if } a \text{ is an ancestor of the perfect annotation and } d(a) < 5
                            \\
                            0.7^{d(a)}, & \text{ if } a \text{ is a descendent of the perfect annotation and } d(a) < 3
                            \\
                            0, & otherwise
                            \end{cases}
                            \)
                            <p>
                                where \(d(a)\) is the depth to the perfect annotation.
                                E.g., \(d(a)=1\) if \(a\) is a parent of the perfect annotation, and \(d(a)=2\) if \(a\)
                                is a grandparent of the perfect annotation.
                            </p>
                        </li>
                        <li>
                            \(
                            max\_annotation\_score(col) =
                            \begin{cases}
                            g(a), & \text{ if } col \text{ has an annotation } a \\
                            0, & \text{ if } col \text{ has no annotation }
                            \end{cases}

                            \)
                        </li>
                        <li>
                            \(AF1\) is used as the primary score, and \(APrecision\) is used as the secondary score.
                        </li>
                        <li>
                            A cell may have multiple equivalent Wikidata items as its GT (e.g., redirected pages
                            Q20514736 and Q852446). For an annotated entity, our evaluator will calculate the score with
                            each GT entity and select the maximum score.
                        </li>
                    </ol>

                    <h5>Submission</h5>
                    Participants can test and develop their systems on the given ground truth (validation set).
                    They can weekly <a href="https://bit.ly/semtab2022-round2" target="_blank"> upload </a> their
                    annotations corresponding to the
                    targets (test set).
                </div>
                <div
                        class="tab-pane fade"
                        id="round2-2t-cea-wd-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round2-2t-cea-wd-tab"
                        tabindex="0"
                >
                    <h4>Cell Entity Annotation by Wikidata (2T-CEA-WD)</h4>
                    <p>
                        This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                        Matching".
                        It is to annotate column cells (entity mentions) in a table with entities of
                        <strong>Wikidata</strong>
                        (version: <a href="https://zenodo.org/record/6643443">20220521</a>) <br>
                        <i class="bi bi-lightbulb-fill" style="color: var(--bs-yellow)"></i>
                        Notes: participants may use the public Wikidata endpoint (or its API) since the above dump is
                        very recent.
                    </p>
                    <h5>Task Description</h5>
                    <p>
                        The task is to annotate each target cell with an entity of Wikidata.
                        Each submission should contain the annotation of the target cell. One cell can be annotated by
                        one entity with the prefix of http://www.wikidata.org/entity/. Any of the equivalent entities of
                        the ground truth entity are regarded as correct. Case is NOT sensitive.
                    </p>
                    <p>
                        The submission file should be in CSV format.
                        Each line should contain the annotation of one cell which is identified by a table id, a column
                        id and a row id.
                        Namely one line should have four fields: "Table ID", "Row ID", "Column ID" and "Entity IRI".
                        Each cell should be annotated by <strong>at most one entity</strong>.
                    </p>
                    <div class="alert alert-warning" role="alert">
                        Use the '<strong>NIL</strong>' annotation when the cell should be annotated with an entity
                        that is <strong>NOT</strong> represented in the KG.
                    </div>
                    <p>
                        The headers should be excluded from the submission file.
                        Here is an example: "OHGI1JNY","32","1","http://www.wikidata.org/entity/Q5484".
                        Please use the prefix of http://www.wikidata.org/entity/ instead of
                        https://www.wikidata.org/wiki/ which is the prefix of the Wikidata page URL.
                    </p>

                    Notes:
                    <ol>
                        <li>
                            Table ID does not include filename extension; make sure you remove the .csv extension from
                            the filename.
                        </li>
                        <li>
                            Column ID is the position of the column in the table file, starting from 0, i.e., first
                            column's ID is 0.
                        </li>
                        <li>
                            Row ID is the position of the row in the table file, starting from 0, i.e., first row's ID
                            is 0.
                        </li>
                        <li>
                            One submission file should have NO duplicate lines for one cell.
                        </li>
                        <li>
                            Annotations for cells out of the target cells are ignored.
                        </li>
                    </ol>


                    <h5>Dataset</h5>
                    <dl class="row">
                        <dt class="col-sm-3">Link</dt>
                        <dd class="col-sm-9"><a
                                href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/ToughTablesR2-WD.tar.gz"
                                target="_blank">Round #2 ToughTables (WD) Dataset</a></dd>

                        <dt class="col-sm-3">Description</dt>
                        <dd class="col-sm-9">
                            <p>The dataset contains:</p>
                            <ul>
                                <li>evaluator codes (CEA_WD_Evaluator.py)</li>
                                <li>the validation set (DataSets/ToughTablesR2-WD/Valid/gt/cea_gt.csv,
                                    DataSets/ToughTablesR2-WD/Valid/tables)
                                </li>
                                <li>the testing set (DataSets/ToughTablesR2-WD/Test/tables,
                                    DataSets/ToughTablesR2-WD/Test/target/cea_target.csv)
                                </li>
                            </ul>
                        </dd>

                        <dt class="col-sm-3">Format</dt>
                        <dd class="col-sm-9">
                            One table is stored in one CSV file. Each line corresponds to a table row. The first row may
                            either be the table header or content. The target cells for annotation are saved in a CSV
                            file.
                        </dd>
                    </dl>

                    <h5>Evaluation Criteria</h5>
                    <p>
                        Precision, Recall and F1 Score are calculated:

                        \[Precision = {{correct\_annotations \#} \over {submitted\_annotations \#}}\]

                        \[Recall = {{correct\_annotations \#} \over {ground\_truth\_annotations \#}}\]

                        \[F1 = {2 \times Precision \times Recall \over Precision + Recall}\]

                    </p>

                    Notes:

                    <ol>
                        <li>
                            # denotes the number.
                        </li>
                        <li>
                            \(F1\) is used as the primary score, and \(Precision\) is used as the secondary score.
                        </li>
                        <li>
                            One target cell, one ground truth annotation, i.e., # ground truth annotations = # target
                            cells. The ground truth annotation has already covered all equivalent entities (e.g., wiki
                            page redirected entities); the ground truth is hit if one of its equivalent entities is hit.
                        </li>
                    </ol>

                    <h5>Submission</h5>
                    Participants can test and develop their systems on the given ground truth (validation set).
                    They can weekly <a href="https://bit.ly/semtab2022-round2" target="_blank"> upload </a> their
                    annotations corresponding to the
                    targets (test set).
                </div>
                <div
                        class="tab-pane fade"
                        id="round2-2t-cta-dbp-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round2-2t-cta-dbp-tab"
                        tabindex="0"
                >
                    <h4>Column Type Annotation by DBpedia (2T-CTA-DBP)</h4>
                    <p>
                        This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                        Matching".
                        It's to annotate an entity column (i.e., a column composed of entity mentions) in a table with
                        types from <strong>DBpedia</strong>
                        (version:
                        <a href="http://downloads.dbpedia.org/wiki-archive/downloads-2016-10.html">2016-10</a>).
                    </p>
                    <h5>Task Description</h5>
                    <p>
                        The task is to annotate each of the given entity columns with classes of DBpedia ontology.
                        Each column can be annotated by multiple types:
                        the one that is as fine grained as possible and correct to all the column cells, is regarded as
                        a <strong>perfect annotation</strong>;
                        the one that is the ancestor of the perfect annotation is regarded as an <strong>okay
                        annotation</strong>;
                        others are regarded as <strong>wrong annotations</strong>.
                    </p>
                    <p>
                        The annotation can be a normal class of DBpedia, with the prefix of
                        http://dbpedia.org/ontology/, such as http://dbpedia.org/ontology/Automobile. Each column
                        should be annotated by <strong>at most one item</strong>.
                        <b>In Round #2 in SemTab 2022, only one annotation (perfect annotation) is scored.</b>
                    </p>
                    <div class="alert alert-warning" role="alert">
                        Use the '<strong>NIL</strong>' annotation when the column should be annotated with a concept
                        that is <strong>NOT</strong> represented in the KG.
                    </div>
                    <p>
                        The annotation should be represented by its full IRI, where the case is NOT sensitive. Each
                        submission should be a CSV file. Each line should include a column identified by table id and
                        column id, and the column's annotation (a DBpedia class). It means one line should include three
                        fields: "Table ID", "Column ID" and "Annotation IRI". The headers should be excluded from the
                        submission file.
                    </p>

                    Notes:
                    <ol>
                        <li>
                            Table ID is the filename of the table data, but does NOT include the extension.
                        </li>
                        <li>
                            Column ID is the position of the column in the input, starting from 0, i.e., first column's
                            ID is 0.
                        </li>
                        <li>
                            One submission file should have NO duplicate lines for each target column.
                        </li>
                        <li>
                            Annotations for columns out of the target columns are ignored.
                        </li>
                    </ol>


                    <h5>Dataset</h5>
                    <dl class="row">
                        <dt class="col-sm-3">Link</dt>
                        <dd class="col-sm-9"><a
                                href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/ToughTablesR2-DBP.tar.gz"
                                target="_blank">Round #2 ToughTables (DBP) Dataset</a></dd>

                        <dt class="col-sm-3">Description</dt>
                        <dd class="col-sm-9">
                            <p>The dataset contains:</p>
                            <ul>
                                <li>evaluator codes (CTA_DBP_Evaluator.py)</li>
                                <li>the validation set (DataSets/ToughTablesR2-DBP/Valid/gt/cta_gt.csv,
                                    DataSets/ToughTablesR2-DBP/Valid/tables)
                                </li>
                                <li>the testing set (DataSets/ToughTablesR2-DBP/Test/tables,
                                    DataSets/ToughTablesR2-DBP/Test/target/cta_gt.csv)
                                </li>
                            </ul>
                        </dd>

                        <dt class="col-sm-3">Format</dt>
                        <dd class="col-sm-9">
                            One table is stored in one CSV file. Each line corresponds to a table row. The
                            first row may either be the table header or content. The target columns for annotation are
                            saved in a CSV file.
                        </dd>
                    </dl>

                    <h5>Evaluation Criteria</h5>
                    <p>
                        Precision, Recall and F1 Score are calculated:

                        \[Precision = {{correct\_annotations \#} \over {submitted\_annotations \#}}\]

                        \[Recall = {{correct\_annotations \#} \over {ground\_truth\_annotations \#}}\]

                        \[F1 = {2 \times Precision \times Recall \over Precision + Recall}\]

                    </p>

                    Notes:

                    <ol>
                        <li>
                            # denotes the number.
                        </li>
                        <li>
                            \(F1\) is used as the primary score, and \(Precision\) is used as the secondary score.
                        </li>
                        <li>
                            One target column, one ground truth annotation, i.e., # ground truth annotations = # target
                            columns. The ground truth annotation has already covered all equivalent entities (e.g., wiki
                            page redirected entities); the ground truth is hit if one of its equivalent entities is hit.
                        </li>
                    </ol>

                    <h5>Submission</h5>
                    Participants can test and develop their systems on the given ground truth (validation set).
                    They can weekly <a href="https://bit.ly/semtab2022-round2" target="_blank"> upload </a> their
                    annotations corresponding to the
                    targets (test set).
                </div>
                <div
                        class="tab-pane fade"
                        id="round2-2t-cea-dbp-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round2-2t-cea-dbp-tab"
                        tabindex="0"
                >
                    <h4>Cell Entity Annotation by DBpedia (2T-CEA-DBP)</h4>
                    <p>
                        This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                        Matching".
                        It is to annotate column cells (entity mentions) in a table with entities of
                        <strong>DBpedia</strong>
                        (version:
                        <a href="http://downloads.dbpedia.org/wiki-archive/downloads-2016-10.html">2016-10</a>).
                    </p>
                    <h5>Task Description</h5>
                    <p>
                        The task is to annotate each target cell with an entity of DBpedia.
                        Each submission should contain the annotation of the target cell. One cell can be annotated by
                        one entity with the prefix of http://dbpedia.org/resource/. Any of the equivalent entities of
                        the ground truth entity are regarded as correct. Case is NOT sensitive.
                    </p>
                    <p>
                        The submission file should be in CSV format.
                        Each line should contain the annotation of one cell which is identified by a table id, a column
                        id and a row id.
                        Namely one line should have four fields: "Table ID", "Row ID", "Column ID" and "Entity IRI".
                        Each cell should be annotated by <strong>at most one entity</strong>.
                    </p>
                    <div class="alert alert-warning" role="alert">
                        Use the '<strong>NIL</strong>' annotation when the cell should be annotated with an entity
                        that is <strong>NOT</strong> represented in the KG.
                    </div>
                    <p>
                        The headers should be excluded from the submission file.
                        Here is an example: "CTRL_WIKI_GEO_list_of_lakes_by_area","1","1","http://dbpedia.org/resource/Caspian_sea".
                        Please use the prefix of http://dbpedia.org/resource/ instead of
                        https://dbpedia.org/page/ which is the prefix of the DBpedia page URL.
                    </p>

                    Notes:
                    <ol>
                        <li>
                            Table ID does not include filename extension; make sure you remove the .csv extension from
                            the filename.
                        </li>
                        <li>
                            Column ID is the position of the column in the table file, starting from 0, i.e., first
                            column's ID is 0.
                        </li>
                        <li>
                            Row ID is the position of the row in the table file, starting from 0, i.e., first row's ID
                            is 0.
                        </li>
                        <li>
                            One submission file should have NO duplicate lines for one cell.
                        </li>
                        <li>
                            Annotations for cells out of the target cells are ignored.
                        </li>
                    </ol>


                    <h5>Dataset</h5>
                    <dl class="row">
                        <dt class="col-sm-3">Link</dt>
                        <dd class="col-sm-9"><a
                                href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/ToughTablesR2-DBP.tar.gz"
                                target="_blank">Round #2 ToughTables (DBP) Dataset</a></dd>

                        <dt class="col-sm-3">Description</dt>
                        <dd class="col-sm-9">
                            <p>The dataset contains:</p>
                            <ul>
                                <li>evaluator codes (CEA_DBP_Evaluator.py)</li>
                                <li>the validation set (DataSets/ToughTablesR2-DBP/Valid/gt/cea_gt.csv,
                                    DataSets/ToughTablesR2-DBP/Valid/tables)
                                </li>
                                <li>the testing set (DataSets/ToughTablesR2-DBP/Test/tables,
                                    DataSets/ToughTablesR2-DBP/Test/target/cea_target.csv)
                                </li>
                            </ul>
                        </dd>

                        <dt class="col-sm-3">Format</dt>
                        <dd class="col-sm-9">
                            One table is stored in one CSV file. Each line corresponds to a table row. The first row may
                            either be the table header or content. The target cells for annotation are saved in a CSV
                            file.
                        </dd>
                    </dl>

                    <h5>Evaluation Criteria</h5>
                    <p>
                        Precision, Recall and F1 Score are calculated:

                        \[Precision = {{correct\_annotations \#} \over {submitted\_annotations \#}}\]

                        \[Recall = {{correct\_annotations \#} \over {ground\_truth\_annotations \#}}\]

                        \[F1 = {2 \times Precision \times Recall \over Precision + Recall}\]

                    </p>

                    Notes:

                    <ol>
                        <li>
                            # denotes the number.
                        </li>
                        <li>
                            \(F1\) is used as the primary score, and \(Precision\) is used as the secondary score.
                        </li>
                        <li>
                            One target cell, one ground truth annotation, i.e., # ground truth annotations = # target
                            cells. The ground truth annotation has already covered all equivalent entities (e.g., wiki
                            page redirected entities); the ground truth is hit if one of its equivalent entities is hit.
                        </li>
                    </ol>

                    <h5>Submission</h5>
                    Participants can test and develop their systems on the given ground truth (validation set).
                    They can weekly <a href="https://bit.ly/semtab2022-round2" target="_blank"> upload </a> their
                    annotations corresponding to the
                    targets (test set).
                </div>
            </div>

        
        <h3 class="pt-4" id="round3">Round #3</h3>
        <ul class="nav nav-tabs mb-3" id="round3-tasks-tab" role="tablist">
            <li class="nav-item" role="presentation">
                <button
                        class="nav-link active"
                        id="round3-gt-cta-dbp-tab"
                        data-bs-toggle="tab"
                        data-bs-target="#round3-gt-cta-dbp-tab-pane"
                        type="button"
                        role="tab"
                        aria-controls="round3-gt-cta-dbp-tab-pane"
                        aria-selected="true"
                >
                    GT-CTA-DBP (Round #3)
                </button>
            </li>
            <li class="nav-item" role="presentation">
                <button
                        class="nav-link"
                        id="round3-gt-cta-sch-tab"
                        data-bs-toggle="tab"
                        data-bs-target="#round3-gt-cta-sch-tab-pane"
                        type="button"
                        role="tab"
                        aria-controls="round3-gt-cta-sch-tab-pane"
                        aria-selected="true"
                >
                    GT-CTA-SCH (Round #3)
                </button>
            </li>
            <li class="nav-item" role="presentation">
                <button
                        class="nav-link"
                        id="round3-biodivtab-cta-dbp-tab"
                        data-bs-toggle="tab"
                        data-bs-target="#round3-biodivtab-cta-dbp-tab-pane"
                        type="button"
                        role="tab"
                        aria-controls="round3-biodivtab-cta-dbp-pane"
                        aria-selected="true"
                >
                    BiodivTab-CTA-DBP (Round #3)
                </button>
            </li>
            <li class="nav-item" role="presentation">
                <button
                        class="nav-link"
                        id="round3-biodivtab-cea-dbp-tab"
                        data-bs-toggle="tab"
                        data-bs-target="#round3-biodivtab-cea-dbp-tab-pane"
                        type="button"
                        role="tab"
                        aria-controls="round3-biodivtab-cea-dbp-pane"
                        aria-selected="true"
                >
                    BiodivTab-CEA-DBP (Round #3)
                </button>
            </li>
        </ul>
        <div class="alert alert-primary d-flex align-items-center" role="alert">
            <i class="bi bi-calendar-event" style="padding-right: 6px;"></i>
            September 19 - October 15
        </div>

        <div class="tab-content" id="round3-tasks-tab-content">
            <div
                class="tab-pane fade show active"
                id="round3-gt-cta-dbp-tab-pane"
                role="tabpanel"
                aria-labelledby="round3-gt-cta-dbp-tab"
                tabindex="0"
            >
                <h4>Column Type Annotation by DBpedia (GT-CTA-DBP)</h4>
                <p>
                    This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                    Matching".
                    It's to annotate an entity column (i.e., a column composed of entity mentions) in a table with
                    types from <strong>DBpedia</strong> (version:
                    <a href="http://downloads.dbpedia.org/wiki-archive/downloads-2016-10.html">2016-10</a>).
                </p>
                <h5>Task Description</h5>
                <p>
                    The task is to annotate each of the given entity columns with properties of the DBpedia ontology.
                    Each column can be annotated by only one type (no ancestors and descendents are taken into account).
                </p>
                <p>
                    The annotation can be a normal property of DBpedia, with the prefix of
                    http://dbpedia.org/ontology/, such as http://dbpedia.org/ontology/Automobile. Each column
                    should be annotated by <strong>at most one item</strong>.
                    <b>In Round #3 in SemTab 2022, only one annotation (perfect annotation) is scored.</b>
                </p>
                <div class="alert alert-warning" role="alert">
                    Use the '<strong>NIL</strong>' annotation when the column should be annotated with a concept
                    that is <strong>NOT</strong> represented in the KG.
                </div>
                <p>
                    The annotation should be represented by its full IRI, where the case is NOT sensitive. Each
                    submission should be a CSV file. Each line should include a column identified by table id and
                    column id, and the column's annotation (a DBpedia class). It means one line should include three
                    fields: "Table ID", "Column ID" and "Annotation URI". The headers should be excluded from the
                    submission file.
                </p>

                Notes for datasets:
                <ol>
                    <li>
                        Table ID is the filename of the table data, but does NOT include the extension (i.e. "csv").
                    </li>
                    <li>
                        Column ID is the position of the column in the input, starting from 0, i.e., first column's
                        ID is 0.
                    </li>
                    <li>
                        One submission file should have NO duplicate lines for each target column.
                    </li>
                    <li>
                        Annotations for columns out of the target columns are ignored.
                    </li>
                </ol>


                <h5>Dataset</h5>
                <dl class="row">
                    <dt class="col-sm-3">Links</dt>
                    <dd class="col-sm-9"><a
                        href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/GitTables_SemTab_2022_dbpedia_dataset.zip"
                        target="_blank">(1) Round #3 GitTables DBpedia - Test Targets, Train Annotations, Evaluation Script</a>
                    </dd>
                    <dt class="col-sm-3"></dt>
                    <dd class="col-sm-9"><a
                        href="https://zenodo.org/record/7091019/files/GitTables_SemTab_2022_tables.zip?download=1"
                        target="_blank">(2) Round #3 GitTables Tables</a>
                    </dd>

                    <dt class="col-sm-3">Description</dt>
                    <dd class="col-sm-9">
                        <p>The datasets provide:</p>
                        <ul>
                            <li>(1) The test targets are in "dbpedia_property_targets.csv" with table ID and target columns associated with properties from dbpedia.
                                <br>
                                The train annotation files "dbpedia_property_train.csv" provides target table ID, target columns,
                                and ground truth labels of associated ontology and URI.
                                <br>
                                The evaluation code in "CTA_DBP_Evaluator.py" that can be used to evaluate a validation split of the train data against the ground truth of DBpedia annotations.
                                It can be used through the CLI by calling <code>python CTA_SCH_Evaluator.py '/path/to/submission/file' '/path/to/ground/truth/file'</code>.
                            </li>
                            <li>(2) All tables for training and testing (GitTables_SemTab_2022_tables.zip).
                            <br>
                            One table is stored in one CSV file. Each line corresponds to a table row. The
                            first row may either be the table header or content. The target columns for annotation are
                            saved in a CSV file.
                            <br>
                            Tables for the DBpedia and Schema.org tasks are the same.

                            </li>
                        </ul>
                    </dd>
                </dl>

                <h5>Evaluation Criteria</h5>
                <p>
                    Precision, Recall and F1 Score are calculated:

                    \[Precision = {{correct\_annotations \#} \over {submitted\_annotations \#}}\]

                    \[Recall = {{correct\_annotations \#} \over {ground\_truth\_annotations \#}}\]

                    \[F1 = {2 \times Precision \times Recall \over Precision + Recall}\]
                </p>

                Notes:
                <ol>
                    <li>
                        # denotes the number.
                    </li>
                    <li>
                        \(F1\) is used as the primary score, and \(Precision\) is used as the secondary score.
                    </li>
                    <li>
                        One target column, one ground truth annotation, i.e., # ground truth annotations = # target
                        columns. The ground truth annotation has already covered all equivalent entities (e.g., wiki
                        page redirected entities); the ground truth is hit if one of its equivalent entities is hit.
                    </li>
                </ol>

                <h5>Submission</h5>

                Participants can test and develop their systems on the given ground truth (validation set).
                They can weekly <a href="https://bit.ly/semtab2022-round3" target="_blank"> upload </a> their
                annotations corresponding to the
                targets (test set).

            </div>

            <div
                class="tab-pane fade"
                id="round3-gt-cta-sch-tab-pane"
                role="tabpanel"
                aria-labelledby="round3-gt-cta-sch-tab"
                tabindex="0"
            >
                <h4>Column Type Annotation by Schema.org (GT-CTA-SCH)</h4>
                <p>
                    This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                    Matching".
                    It's to annotate an entity column (i.e., a column composed of entity mentions) in a table with
                    types from <strong>Schema.org</strong> properties <strong>or</strong> classes.
                </p>
                <h5>Task Description</h5>
                <p>
                    The task is to annotate each of the given entity columns with properties or classes from Schema.org.
                    Each column can be annotated by only one type (no ancestors and descendents are taken into account).
                </p>
                <p>
                    The annotation can be a normal property/class of Schame.org, with the prefix of
                    https://schema.org/, such as https://schema.org/identifier. Each column
                    should be annotated by <strong>at most one item</strong>.
                    <b>In Round #3 in SemTab 2022, only one annotation (perfect annotation) is scored.</b>
                </p>
                <div class="alert alert-warning" role="alert">
                    Use the '<strong>NIL</strong>' annotation when the column should be annotated with a concept
                    that is <strong>NOT</strong> represented in the KG.
                </div>
                <p>
                    The annotation should be represented by its full IRI, where the case is NOT sensitive. Each
                    submission should be a CSV file. Each line should include a column identified by table id and
                    column id, and the column's annotation (a DBpedia class). It means one line should include three
                    fields: "Table ID", "Column ID" and "Annotation URI". The headers should be excluded from the
                    submission file.
                </p>

                Notes for datasets:
                <ol>
                    <li>
                        Table ID is the filename of the table data, but does NOT include the extension (i.e. "csv").
                    </li>
                    <li>
                        Column ID is the position of the column in the input, starting from 0, i.e., first column's
                        ID is 0.
                    </li>
                    <li>
                        One submission file should have NO duplicate lines for each target column.
                    </li>
                    <li>
                        Annotations for columns out of the target columns are ignored.
                    </li>
                </ol>


                <h5>Dataset</h5>
                <dl class="row">
                    <dt class="col-sm-3">Links</dt>
                    <dd class="col-sm-9"><a
                        href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/GitTables_SemTab_2022_schema_dataset.zip"
                        target="_blank">(1) Round #3 GitTables Schema.org - Test Targets, Train Annotations, Evaluation Script</a>
                    </dd>
                    <dt class="col-sm-3"></dt>
                    <dd class="col-sm-9"><a
                        href="https://zenodo.org/record/7091019/files/GitTables_SemTab_2022_tables.zip?download=1"
                        target="_blank">(2) Round #3 GitTables Tables</a>
                    </dd>

                    <dt class="col-sm-3">Description</dt>
                    <dd class="col-sm-9">
                        <p>The datasets provide:</p>
                        <ul>
                            <li>(1) The test targets are in "schema_property_targets.csv" and "schema_class_targets.csv" with table ID and columns associated with properties and classes from schema.org.
                                <br>
                                The train annotation files "schema_property_train.csv" and "schema_class_train.csv" provides target table ID, target columns,
                                and ground truth labels of associated ontology and URI.
                                <br>
                                The evaluation code in "CTA_SCH_Evaluator.py" that can be used to evaluate a validation split of the train data against the ground truth of Schema.org annotations.
                                It can be used through the CLI by calling <code>python CTA_SCH_Evaluator.py '/path/to/submission/file' '/path/to/ground/truth/file'</code>.
                            </li>
                            <li>(2) All tables for training and testing (GitTables_SemTab_2022_tables.zip).
                            <br>
                            One table is stored in one CSV file. Each line corresponds to a table row. The
                            first row may either be the table header or content. The target columns for annotation are
                            saved in a CSV file.
                            <br>
                            Tables for Schema.org and DBpedia are the same.

                            </li>
                        </ul>
                    </dd>
                </dl>

                <h5>Evaluation Criteria</h5>
                <p>
                    Precision, Recall and F1 Score are calculated:

                    \[Precision = {{correct\_annotations \#} \over {submitted\_annotations \#}}\]

                    \[Recall = {{correct\_annotations \#} \over {ground\_truth\_annotations \#}}\]

                    \[F1 = {2 \times Precision \times Recall \over Precision + Recall}\]
                </p>

                Notes:
                <ol>
                    <li>
                        # denotes the number.
                    </li>
                    <li>
                        \(F1\) is used as the primary score, and \(Precision\) is used as the secondary score.
                    </li>
                    <li>
                        One target column, one ground truth annotation, i.e., # ground truth annotations = # target
                        columns. The ground truth annotation has already covered all equivalent entities (e.g., wiki
                        page redirected entities); the ground truth is hit if one of its equivalent entities is hit.
                    </li>
                </ol>

                <h5>Submission</h5>

                Make sure it is clear from the filename if the submission is for the property or the class subsets.
                <br>
                Participants can test and develop their systems on the given ground truth (validation set).
                They can weekly <a href="https://bit.ly/semtab2022-round3" target="_blank"> upload </a> their
                annotations corresponding to the targets (test set).
            </div>

            <div
                        class="tab-pane fade"
                        id="round3-biodivtab-cta-dbp-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round3-biodivtab-cta-dbp-tab"
                        tabindex="0"
                >
                    <h4>Column Type Annotation by DBpedia (BiodivTab-CTA-DBP)</h4>
                    <p>
                        This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                        Matching".
                        It's to annotate an entity column (i.e., a column composed of entity mentions) in a table with
                        types from the <strong>DBpedia</strong> (version:
                        <a href="https://databus.dbpedia.org/dbpedia/collections/dbpedia-snapshot-2022-03">2022-03</a>).
                    </p>
                    <h5>Task Description</h5>
                    <p>
                        The task is to annotate each of the given entity columns with instances <strong>or</strong> classes of DBpedia ontology.
                        Each column can be annotated by <strong>exactly one</strong> type
                        the one that is as fine grained as possible and correct to all the column cells.
                    </p>
                    <div class="alert alert-warning" role="alert">
                        Use the '<strong>NIL</strong>' annotation when the column should be annotated with a concept
                        that is <strong>NOT</strong> represented in the KG/DBpedia.
                    </div>
                    <p>
                        The annotation should be represented by its full IRI, where the case is NOT sensitive. Each
                        submission should be a CSV file. Each line should include a column identified by table id and
                        column id, and the column's annotation (a DBpedia instance/class). It means one line should include three
                        fields: "Table ID", "Column ID" and "Annotation IRI". The headers should be excluded from the
                        submission file.
                    </p>

                    Notes:
                    <ol>
                        <li>
                            Table ID is the filename of the table data, but does NOT include the extension.
                        </li>
                        <li>
                            Column ID is the position of the column in the input, starting from 0, i.e., first column's
                            ID is 0.
                        </li>
                        <li>
                            One submission file should have NO duplicate lines for each target column.
                        </li>
                        <li>
                            Annotations for columns out of the target columns are ignored.
                        </li>
                    </ol>


                    <h5>Dataset</h5>
                    <dl class="row">
                        <dt class="col-sm-3">Link</dt>
                        <dd class="col-sm-9"><a
                                href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/BiodivTab_DBP_2022.zip"
                                target="_blank">Round #3 BiodivTab (DBP) Dataset</a></dd>

                        <dt class="col-sm-3">Description</dt>
                        <dd class="col-sm-9">
                            <p>The dataset contains:</p>
                            <ul>
                                <li>evaluator codes (evaluator/cta_evaluator.py)</li>
                                <li>the validation set (val/tables,
                                    val/targets/CTA_biodivtab_targets.csv,
                                    val/gt/CTA_biodivtab_gt.csv)
                                </li>
                                <li>the testing set (test/tables,
                                    test/targets/gt/CTA_biodivtab_targets.csv)
                                </li>
                            </ul>
                        </dd>

                        <dt class="col-sm-3">Format</dt>
                        <dd class="col-sm-9">
                            One table is stored in one CSV file. Each line corresponds to a table row. The
                            first row may either be the table header or content. The target columns for annotation are
                            saved in a CSV file.
                        </dd>
                    </dl>

                    <h5>Evaluation Criteria</h5>
                    <p>
                        Precision, Recall and F1 Score are calculated:

                        \[Precision = {{correct\_annotations \#} \over {submitted\_annotations \#}}\]

                        \[Recall = {{correct\_annotations \#} \over {ground\_truth\_annotations \#}}\]

                        \[F1 = {2 \times Precision \times Recall \over Precision + Recall}\]

                    </p>

                    Notes:

                    <ol>
                        <li>
                            # denotes the number.
                        </li>
                        <li>
                            \(F1\) is used as the primary score, and \(Precision\) is used as the secondary score.
                        </li>
                        <li>
                            One target column, one ground truth annotation, i.e., # ground truth annotations = # target
                            columns.
                        </li>
                    </ol>

                    <h5>Submission</h5>
                    Participants can test and develop their systems on the given ground truth (validation set).
                    They can weekly <a href="#" target="_blank"> upload </a> their
                    annotations corresponding to the
                    targets (test set).
                </div>

        </div>
        
        <div
            class="tab-pane fade"
            id="round3-biodivtab-cea-dbp-tab-pane"
            role="tabpanel"
            aria-labelledby="round3-biodivtab-cea-dbp-tab"
            tabindex="0"
        >
            <h4>Entity Type Annotation by DBpedia (BiodivTab-CEA-DBP)</h4>
            <p>
                This is a task of ISWC 2022 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                Matching".
                It's to annotate an entity cell in a table with
                instances from <strong>DBpedia</strong> entities <strong>http://dbpedia.org/resource/[ENTITY]</strong>.
                (version: <a href="https://databus.dbpedia.org/dbpedia/collections/dbpedia-snapshot-2022-03">2022-03</a>).

            </p>
            <h5>Task Description</h5>
            <p>
                The task is to annotate each of the given entity cells with instances from DBpedia.
                Each cell can be annotated by only one instance.
            </p>
            <div class="alert alert-warning" role="alert">
                Use the '<strong>NIL</strong>' annotation when a cell should be annotated with an entity
                that is <strong>NOT</strong> represented in the KG/DBpedia.
            </div>
            <p>
                The annotation should be represented by its full IRI, where the case is NOT sensitive. Each
                submission should be a CSV file. Each line should include a column identified by table id and
                column id, row id, and the column's annotation (a DBpedia instance).
                It means one line should include three fields: "Table ID", "Column ID" and "Annotation URI".
                The headers should be excluded from the submission file.
            </p>

             <h5>Dataset</h5>
                <dl class="row">
                    <dt class="col-sm-3">Link</dt>
                    <dd class="col-sm-9"><a
                            href="https://github.com/sem-tab-challenge/2022/blob/main/datasets/BiodivTab_DBP_2022.zip"
                            target="_blank">Round #3 BiodivTab (DBP) Dataset</a></dd>

                    <dt class="col-sm-3">Description</dt>
                    <dd class="col-sm-9">
                        <p>The dataset contains:</p>
                        <ul>
                            <li>evaluator codes (evaluator/cea_evaluator.py)</li>
                            <li>the validation set (val/tables,
                                    val/targets/CEA_biodivtab_targets.csv,
                                    val/gt/CEA_biodivtab_gt.csv,
                                )
                            </li>
                            <li>the testing set (test/tables,
                                test/targets/CEA_biodivtab_targets.csv)
                            </li>
                        </ul>
                    </dd>

                    <dt class="col-sm-3">Format</dt>
                    <dd class="col-sm-9">
                        One table is stored in one CSV file. Each line corresponds to a table row. The first row may
                        either be the table header or content. The target cells for annotation are saved in a CSV
                        file.
                    </dd>
                </dl>

               <h5>Evaluation Criteria</h5>
                <p>
                    Precision, Recall and F1 Score are calculated:

                    \[Precision = {{correct\_annotations \#} \over {submitted\_annotations \#}}\]

                    \[Recall = {{correct\_annotations \#} \over {ground\_truth\_annotations \#}}\]

                    \[F1 = {2 \times Precision \times Recall \over Precision + Recall}\]

                </p>

                Notes:

                <ol>
                    <li>
                        # denotes the number.
                    </li>
                    <li>
                        \(F1\) is used as the primary score, and \(Precision\) is used as the secondary score.
                    </li>
                    <li>
                        One target cell, one ground truth annotation, i.e., # ground truth annotations = # target
                        cells. The ground truth annotation has already covered all equivalent entities (e.g., wiki
                        page redirected entities); the ground truth is hit if one of its equivalent entities is hit.
                    </li>
                </ol>

            <h5>Submission</h5>

            Make sure it is clear from the filename if the submission is for the property or the class subsets.
            <br>
            Participants can test and develop their systems on the given ground truth (validation set).
            They can weekly <a href="#" target="_blank"> upload </a> their
            annotations corresponding to the targets (test set).
        </div>

        <br>
        
        <!--h2 class="pt-4" id="results">Testing Results</h2-->
        <h2 class="title display-6" id="results">
            <i class="bi bi-trophy"></i>
            Testing Results
        </h2>
        
                    
        <div class="alert alert-primary d-flex align-items-center" role="alert">
            <a href="https://docs.google.com/spreadsheets/d/1DgX4JX1aQkDNrRSW4l54yFT7zdnnI6EKanXvGoXww9I/edit?usp=sharing">Round
                #1</a>
        </div>
        
        <div class="alert alert-primary d-flex align-items-center" role="alert">
            <a href="https://docs.google.com/spreadsheets/d/16prYkaqGdvF0VpaYsmswe82_6kdOd0jNiIOjhp3EIOQ/edit?usp=sharing">Round #2</a>
        </div>

        <!--
        <h4>Round 1</h4>
        <ul>
        <li><b>Knowledge Graphs:</b> DBPedia (version: <a href="http://downloads.dbpedia.org/wiki-archive/downloads-2016-10.html">2016-10</a>) and Wikidata (version: <a href="https://doi.org/10.5281/zenodo.6153449">20210828</a>)</li>
        <li><b>Datasets and targets:</b> <a href="./data/tables_CTA_CEA_Round1.tar.gz" target="_blank">tables of CTA-DBP and CEA-DBP</a>, <a href="./data/CTA_DBP_Round1_Targets.csv" target="_blank">CTA-DBP targets</a>, <a href="./data/CEA_DBP_Round1_Targets.csv" target="_blank">CEA-DBP targets</a>, <a href="./data/tables_CTA_CEA_WD_Round1.tar.gz" target="_blank">tables of CTA-WD and CEA-WD</a>, <a href="./data/CTA_WD_Round1_Targets.csv">CTA-WD targets</a>, <a href="./data/CEA_WD_Round1_Targets.csv">CEA-WD targets</a>.</li>
        <li><b>CTA-DBP Task</b>: Assigning a DBPedia semantic type (a DBpedia class as fine-grained as possible) to a column. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/column-type-annotation-by-dbpedia-cta-dbp">See AIcrowd page</a>.</li>
        <li><b>CEA-DBP Task</b>: Matching a cell to a DBpedia entity. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/cell-entity-annotation-by-dbpedia-cea-dbp">See AIcrowd page</a>.</li>
        <li><b>CTA-WD Task</b>: Assigning a Wikidata semantic type (a Wikidata entity as fine-grained as possible) to a column. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/column-type-annotation-by-wikidata-cta-wd">See AIcrowd page</a>.</li></li>
        <li><b>CEA-WD Task</b>: Matching a cell to a Wikidata entity. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/cell-entity-annotation-by-wikidata-cea-wd">See AIcrowd page</a></li>
        </ul>

        <br>

        <h4>Round 2: </h4>
        <ul>
        <li><b>Knowledge Graphs:</b> Wikidata (version: <a href="https://doi.org/10.5281/zenodo.6153449">20210828</a>)</li>
        <li><b>BioTable Datasets and targets:</b> <a href="./data/BioTable_CTA_CEA_CPA_WD_Round2.tar.gz" target="_blank">tables of BioTable-CTA-WD, BioTable-CEA-WD and BioTable-CPA-WD</a>, <a href="./data/BioTable_CTA_WD_Round2_Targets.csv" target="_blank">BioTable-CTA-WD targets</a>, <a href="./data/BioTable_CEA_WD_Round2_Targets.csv" target="_blank">BioTable-CEA-WD targets</a>, <a href="./data/BioTable_CPA_WD_Round2_Targets.csv" target="_blank">BioTable-CPA-WD targets</a>.</li>
        <li><b>BioTable-CTA-WD Task</b>: Assigning a Wikidata semantic type (a Wikidata entity as fine-grained as possible) to a column. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/biotable-column-type-annotation-by-wikidata-biotable-cta-wd">See AIcrowd page</a>.</li>
        <li><b>BioTable-CEA-WD Task</b>: Assigning a Wikidata entity to a cell. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/biotable-cell-entity-annotation-by-wikidata-biotable-cea-wd">See AIcrowd page</a>.</li>
        <li><b>BioTable-CPA-WD Task</b>: Assigning a Wikidata property to a column pair (order matters). <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/biotable-column-property-annotation-by-wikidata-biotable-cpa-wd">See AIcrowd page</a>.</li>
        <li><b>AG (HardTable) Datasets and targets:</b> <a href="./data/HardTable_CTA_CEA_CPA_WD_Round2.tar.gz" target="_blank">tables of HardTable-CTA-WD, HardTable-CEA-WD and HardTable-CPA-WD</a>, <a href="./data/HardTable_CTA_WD_Round2_Targets.csv" target="_blank">HardTable-CTA-WD targets</a>, <a href="./data/HardTable_CEA_WD_Round2_Targets.csv" target="_blank">HardTable-CEA-WD targets</a>, <a href="./data/HardTable_CPA_WD_Round2_Targets.csv" target="_blank">HardTable-CPA-WD targets</a>.</li>
        <li><b>HardTable-CTA-WD Task</b>: Assigning a Wikidata semantic type (a Wikidata entity as fine-grained as possible) to a column. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/hardtable-column-type-annotation-by-wikidata-hardtable-cta-wd">See AIcrowd page</a>.</li>
        <li><b>HardTable-CEA-WD Task</b>: Assigning a Wikidata entity to a cell. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/hardtable-cell-entity-annotation-by-wikidata-hardtable-cea-wd">See AIcrowd page</a>.</li>
        <li><b>HardTable-CPA-WD Task</b>: Assigning a Wikidata property to a column pair (order matters). <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/hardtable-column-property-annotation-by-wikidata-hardtable-cpa-wd">See AIcrowd page</a>.</li>
        </ul>

        <br>

        <h4> Round 3: </h4>
        <ul>
        <li><b>Knowledge Graphs:</b> Schema.org (version: <a href="https://gittables.github.io/">May 2021</a>),  DBPedia (version: <a href="http://downloads.dbpedia.org/wiki-archive/downloads-2016-10.html">2016-10</a>), Wikidata (version: <a href="https://doi.org/10.5281/zenodo.6153449">20210828</a>)</li>
        <li><b>BioDivTab Datasets and targets:</b> <a href="./data/BioDivTab_CTA_CEA_WD_Round3.tar.gz" target="_blank">tables of BioDivTab-CTA-WD and BioDivTab-CEA-WD</a>, <a href="./data/BioDivTab_CTA_WD_Round3_Targets.csv" target="_blank">BioDivTab-CTA-WD targets</a>, <a href="./data/BioDivTab_CEA_WD_Round3_Targets.csv" target="_blank">BioDivTab-CEA-WD targets</a>. (Knowledge Graph: "Live Edition" of Wikidata)</li>
        <li><b>GitTables Datasets and targets:</b> <a href="data/GitTables_CTA_DBP_SCH_Round3.tar.gz" target="_blank">tables of GitTables-CTA-DBP and GitTables-CTA-SCH (one column by one Schema.org class such as schema:author and schema:URL)</a>, <a href="./data/GitTables_CTA_DBP_Round3_Targets.csv" target="_blank">GitTables-CTA-DBP targets</a>, <a href="./data/GitTables_CTA_DBP_Round3_Labels.csv">GitTables-CTA-DBP labels (including DBpedia properties)</a>, <a href="data/GitTables_CTA_SCH_Round3_Targets.csv" target="_blank">GitTables-CTA-SCH targets</a>, <a href="./data/GitTables_CTA_SCH_Round3_Labels.csv">GitTables-CTA-SCH labels (including properties and types from Schema.org)</a> </li>
        <li><b>AG (HardTable) R3 Datasets and targets:</b> <a href="./data/HardTablesR3_CTA_CEA_CPA_WD_Round3.tar.gz" target="_blank">tables of HardTablesR3-CTA-WD, HardTablesR3-CEA-WD and HardTablesR3-CPA-WD</a>, <a href="./data/HardTablesR3_CTA_WD_Round3_Targets.csv" target="_blank">HardTablesR3-CTA-WD targets</a>, <a href="./data/HardTablesR3_CEA_WD_Round3_Targets.csv" target="_blank">HardTablesR3-CEA-WD targets</a>, <a href="./data/HardTablesR3_CPA_WD_Round3_Targets.csv" target="_blank">HardTablesR3-CPA-WD targets</a>. (Knowledge Graph: Wikidata (version: <a href="https://dumps.wikimedia.org/wikidatawiki/entities/20210823/">20210823</a>))</li>
        </ul> -->

        <!-- <h3>Usability Track</h3>
        This new track addresses a pain point in the community regarding a lack of publicly available easy-to-use and generic solution that will address the needs of a variety of applications and settings.
        We will devise a clear scoring mechanism to rank every participant's solution in terms of several usability criteria as judged by a review panel, for example:
        <ol>
        <li>Is the solution open-source?</li>
        <li>Does the solution require specific platform that could affect its use in common settings?</li>
        <li>Does the solution require extensive training and tuning for a new application/domain?</li>
        <li>Is the solution offered as a public service?</li>
        <li>Does the solution include a well-designed user interface?</li>
        </ol> -->

            <!-- <h3>Applications Track</h3>

        <p>This new track aims at addressing applications in real-world settings that take advantage of the output of the matching systems.
        Challenging dataset proposals are also more than welcome.</p>


        Examples include but are not limited to:
        <ol>
        <li>Applications in generic data discovery and exploration.</li>
        <li>Applications of table understanding for scientific corpora.</li>
        <li>Applications in feature engineering and automated machine learning.</li>
        </ol> -->

            <!-- <p>
        <b>Bio-Track:</b> Due to advances in biological research techniques, new data is constantly being produced in the biomedical domain and it is commonly published unstructured or tabular formats. This data is not trivial to integrate semantically due not only to its sheer amount but also the complexity of the biological relations between entities. Specifically, for tabular data annotation, the representation of data can have a significant impact in performance since each entity can be represented by alphanumeric codes (e.g., chemical formulas or gene names) or even have multiple synonyms. Therefore, the domain would greatly benefit from automated methods to map entities, entity types and properties to existing datasets to speed-up the process of integrating new data in the domain.
        </p> -->
            <!--
            <br /> -->

            <!--
        <li><b>October 21:</b> System paper submissions (via <a href="https://easychair.org/conferences/?conf=semtab2022" target="_blank">easychair</a>).</li>
        <li><b>October 25:</b> <a href="http://om2022.ontologymatching.org/" target="_blank">Ontology Matching workshop</a>.</li>
        <li><b>October 23-27:</b> <a href="https://iswc2022.semanticweb.org/" target="_blank">Challenge Presentation</a> and prize announcement.</li>
        <li><b>November 15:</b> Final version system papers (via <a href="https://easychair.org/conferences/?conf=semtab2022" target="_blank">easychair</a>).</li>
         -->

            <h2 class="title display-6 pt-5" id="paper">
                <i class="bi bi-journal-check"></i>
                Paper Guidelines
            </h2>

            <p>
                We invite participants in the Accuracy Track as well as the Datasets
                Track to submit a paper using <a
                    href="https://easychair.org/my/conference?conf=semtab2022">easychair</a>.<br/>
                System papers in the Accuracy Track should be no more than 12 pages
                long (excluding references) and papers for the Datasets Track are
                limited to 6 pages.<br/> If you are submitting to the Datasets Track, please append "[Datasets Track]"
                at the end of the paper title. <br/>
                Both type of papers should be formatted using the
                <a
                        href="https://www.overleaf.com/latex/templates/template-for-submissions-to-ceur-workshop-proceedings-ceur-ws-dot-org/wqyfdgftmcfw"
                        target="_blank"
                >CEUR Latex template</a
                >
                or the
                <a
                        href="https://ceurws.wordpress.com/2020/03/31/ceurws-publishes-ceurart-paper-style/"
                        target="_blank"
                >CEUR Word template</a
                >. Papers will be reviewed by 1-2 challenge organisers.
            </p>

            <p>
                Accepted papers will be published as a volume of
                <a href="http://ceur-ws.org/" target="_blank">CEUR-WS</a>. By
                submitting a paper, the authors accept the CEUR-WS publishing rules.
            </p>

            <h2 id="organisation" class="title display-6 pt-5">
                <i class="bi bi-people"></i>
                Organisation
            </h2>

            <p>
                This challenge is organised by
                <a
                        href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-Kavitha.Srinivas"
                        target="_blank"
                >Kavitha Srinivas</a
                >
                (IBM Research),
                <a
                        href="https://www.city.ac.uk/people/academics/ernesto-jimenez-ruiz"
                        target="_blank"
                >Ernesto Jim&eacute;nez-Ruiz</a
                >
                (City, University of London; University of Oslo),
                <a
                        href="https://researcher.watson.ibm.com/researcher/view.php?person=us-hassanzadeh"
                        target="_blank"
                >Oktie Hassanzadeh</a
                >
                (IBM Research),
                <a
                        href="https://www.cs.ox.ac.uk/people/jiaoyan.chen/"
                        target="_blank"
                >Jiaoyan Chen</a
                >
                (University of Oxford),
                <a href="https://sites.google.com/site/vefthym/" target="_blank"
                >Vasilis Efthymiou</a
                >
                (FORTH - ICS),
                <a href="https://vcutrona.github.io/" target="_blank"
                >Vincenzo Cutrona</a
                >
                (SUPSI),
                <a href="https://juansequeda.com/" target="_blank">Juan Sequeda</a>
                (data.world),
                <!--<a href="https://danielapoliveira.github.io/" target="_blank">Daniela Oliveira</a> (Universidade de Lisboa),
          <a href="http://www.di.fc.ul.pt/~catiapesquita/" target="_blank">Catia Pesquita</a> (Universidade de Lisboa),
          -->
                <a
                        href="https://fusion.cs.uni-jena.de/fusion/members/nora-abdelmageed/"
                        target="_blank"
                >Nora Abdelmageed</a
                >
                (University of Jena), and
                <a href="https://madelonhulsebos.github.io/" target="_blank"
                >Madelon Hulsebos</a
                >
                (Sigma Computing, University of Amsterdam). If you have any problems
                working with the datasets or any suggestions related to this
                challenge, do not hesitate to contact us via the
                <a href="https://groups.google.com/g/sem-tab-challenge"
                >discussion group</a
                >.
            </p>

            <h2 id="acknowledgements" class="title display-6 pt-5">
                <i class="bi bi-bookmark-star"></i>
                Acknowledgements
            </h2>

            <p>
                The challenge is currently supported by the
                <a href="http://sirius-labs.no/" target="_blank"
                >SIRIUS Centre for Research-driven Innovation</a
                >
                and
                <a href="http://www.research.ibm.com/" target="_blank"
                >IBM Research</a
                >.
            </p>

            <!--<p>-->
            <!--BiodivTab is credited to Nora Abdelmageed, Sirko Schindler, Birgitta K&ouml;nig-Ries, Heinz Nixdorf Chair for Distributed Information Systems, Friedrich Schiller University Jena, Germany.-->
            <!--The tables provided in this challenge are based on real biodiversity research datasets, but have been adapted for the challenge. In the form provided here, they may be used for the challenge, only. -->
            <!--</p>-->

            <!--<p>-->
            <!--Any publication on challenge results needs to contain citations of the underlying datasets.  -->
            <!--</p>-->

            <!-- <br /> -->


        </div>
        <div class="col-md-2 col-3 vstack gap-3">
            <a href="http://sirius-labs.no/" target="_blank">
                <img src="logos/sirius-logo.png" class="img-fluid float-lg-end"
                     alt="sirius" width="250"/></a>

            <a href="http://www.cs.ox.ac.uk/" target="_blank">
                <img src="logos/cs-oxford.jpg" class="img-fluid float-lg-end"
                     alt="cs-oxford" width="250"/></a>

            <a
                    href="https://www.city.ac.uk/about/schools/mathematics-computer-science-engineering/computer-science"
                    target="_blank"
            >
                <img src="logos/city-logo.jpg" class="img-fluid float-lg-end"
                     alt="city-logo" width="250"/></a>

            <a href="https://www.ibm.com/" target="_blank">
                <img src="logos/ibm-logo-small.svg" class="img-fluid float-lg-end" width="250"/></a>

            <a href="https://www.ics.forth.gr/" target="_blank">
                <img src="logos/ics-forth.jpg" class="img-fluid float-lg-end"
                     alt="ics-forth" width="250"/></a>

            <a href="https://data.world/" target="_blank">
                <img src="logos/data-world.png" class="img-fluid float-lg-end" width="250"/></a>

            <a href="https://www.sigmacomputing.com" target="_blank">
                <img src="logos/sigma.png" class="img-fluid float-lg-end"
                     alt="sigma" width="250"/></a>

            <a href="https://indelab.org" target="_blank">
                <img src="logos/uva.png" class="img-fluid float-lg-end"
                     alt="uva" width="250"/></a>

            <a href="https://www.uni-jena.de/en" target="_blank">
                <img src="logos/jena.png" class="img-fluid float-lg-end"
                     alt="jena" width="250"/></a>

            <a href="https://iswc2021.semanticweb.org/" target="_blank">
                <img src="logos/iswc2022.png" class="img-fluid float-lg-end"
                     alt="iswc2022" width="250"/></a>
        </div>
    </div>
</div>
<footer>

</footer>

<script
        src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-pprn3073KE6tl6bjs2QrFaJGz5/SUsLqktiwsUTF55Jfv3qYSDhgCecCxMW52nD2"
        crossorigin="anonymous"
></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
</body>
</html>
