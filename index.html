<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>

    <title>Semantic Web Challenge on Tabular Data to Knowledge Graph Matching</title>
    <link rel="stylesheet" type="text/css" href="style.css"/>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-0evHe/X+R7YkIZDRvuzKMRqM+OrBnVFBL6DOitfPri4tjfHxaWutUpFmBp4vmVor"
          crossorigin="anonymous"/>
</head>
<body>
<nav id="navbar" class="navbar navbar-expand-lg bg-light sticky-top">
    <div class="container-fluid">
        <span class="navbar-brand mb-0 h1">SemTab 2022</span>
        <button class="navbar-toggler"
                type="button"
                data-bs-toggle="collapse"
                data-bs-target="#navbarSupportedContent"
                aria-controls="navbarSupportedContent"
                aria-expanded="false"
                aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                <li class="nav-item">
                    <a class="nav-link" href="#">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#forum">Participate!</a>
                </li>
                <li class="nav-item dropdown">
                    <a class="nav-link dropdown-toggle"
                       href="#"
                       id="navbarDropdown"
                       role="button"
                       data-bs-toggle="dropdown"
                       aria-expanded="false">
                        Tracks
                    </a>
                    <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
                        <li>
                            <a class="dropdown-item" href="#accuracy-track">Accuracy Track</a>
                        </li>
                        <li>
                            <a class="dropdown-item" href="#datasets-track">Datasets Track</a>
                        </li>
                        <li>
                            <a class="dropdown-item" href="#artifacts-track">Artifacts Availability Badge</a>
                        </li>
                    </ul>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#paper">Paper Guidelines</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#organisation">Organisation</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#acknowledgements">Acknowledgements</a>
                </li>
            </ul>
            <span class="navbar-text">Semantic Web Challenge on Tabular Data to Knowledge Graph Matching</span>
        </div>
    </div>
</nav>
<!--<div class="alert alert-warning" role="alert">-->
<!--  <b>News (09/03/2022):</b> The <a class="alert-link" href="http://ceur-ws.org/Vol-3103/" target="_blank">SemTab 2021 proceedings</a> are out. <a href="#results" class="alert-link">Results</a> and <a href="#gt" class="alert-link">ground</a> truths are available.-->
<!--</div>-->
<div class="container-fluid">
    <div class="row">
        <div class="col-md-10 col-9"
             data-bs-spy="scroll"
             data-bs-target="#navbar"
             data-bs-root-margin="0px 0px -40%"
             data-bs-smooth-scroll="true"
             tabindex="0">
            <h1>
                SemTab 2022: Semantic Web Challenge on Tabular Data to Knowledge
                Graph Matching
            </h1>

            <p>
                Tabular data in the form of CSV files is the common input format in
                a data analytics pipeline. However, a lack of understanding of the
                semantic structure and meaning of the content may hinder the data
                analytics process. Thus gaining this semantic understanding will be
                very valuable for data integration, data cleaning, data mining,
                machine learning and knowledge discovery tasks. For example,
                understanding what the data is can help assess what sorts of
                transformation are appropriate on the data.
            </p>

            <p>
                Tables on the Web may also be the source of highly valuable data.
                The addition of semantic information to Web tables may enhance a
                wide range of applications, such as web search, question answering,
                and knowledge base (KB) construction.
            </p>

            <p>
                Tabular data to Knowledge Graph (KG) matching is the process of
                assigning semantic tags from Knowledge Graphs (e.g., Wikidata or
                DBpedia) to the elements of the table. This task however is often
                difficult in practice due to metadata (e.g., table and column names)
                being missing, incomplete or ambiguous.
            </p>

            <p>
                The <a href="http://www.cs.ox.ac.uk/isg/challenges/sem-tab/">SemTab challenge</a>
                aims at benchmarking systems dealing with the tabular data to KG
                matching problem, so as to facilitate their comparison on the same
                basis and the reproducibility of the results.
            </p>

            <p>
                The <b>2022 edition</b> of this challenge will be collocated with the
                <a href="https://iswc2022.semanticweb.org/" target="_blank">
                    21st International Semantic Web Conference
                </a>
                and the
                <a href="http://om2022.ontologymatching.org/" target="_blank"
                >17th International Workshop on Ontology Matching</a
                >.
            </p>

            <h2 class="pt-5" id="forum">Participation: forum and registration</h2>
            <hr/>

            <p>
                We have a
                <a href="https://groups.google.com/d/forum/sem-tab-challenge" target="_blank">discussion group</a>
                for the challenge where we share the latest news with the
                participants and we discuss issues risen during the evaluation
                rounds.
            </p>

            <p>
                Please register your system using this
                <a href="https://bit.ly/semtab2022-participation" target="_blank">google form</a>.
            </p>

            <p>
                Note that participants can join SemTab at any Round for any of the
                tasks/tracks.
            </p>

            <h2 class="pt-5" id="tasks">Challenge Tracks</h2>
            <hr/>

            <h3 id="accuracy-track">Accuracy Track</h3>

            The evaluation of systems regarding accuracy is similar to prior
            versions of the SemTab.<br/>
            That is, to illustrate the accuracy of the submissions, we evaluate
            systems on typical multi-class classification metrics as detailed
            below.<br/>
            In addition, we adopt the "cscore" for the CTA task to reflect the
            distance in the type hierarchy between the predicted column type and
            the ground truth semantic type.<br/>
            We use AICrowd to collect the submissions and automatically conduct
            the evaluation, the page on AICrowd will provide more details
            regarding evaluation.<br/>
            <br/>
            Matching Tasks:
            <ul>
                <li>
                    <b>CTA Task</b>: Assigning a semantic type (a DBpedia class as
                    fine-grained as possible) to a column.
                </li>
                <li><b>CEA Task</b>: Matching a cell to a Wikidata entity.</li>
                <li>
                    <b>CPA Task</b>: Assigning a KG property to the relationship
                    between two columns.
                </li>
            </ul>

            Matching Criteria:
            <ul>
                <li>Average Precision</li>
                <li>Average Recall</li>
                <li>Average F1</li>
                <li>Cscore</li>
            </ul>

            Important Dates (all 2022):
            <ul>
                <li><b>May 26:</b> First call for challenge participants.</li>
                <li><b>June 13 - July 13:</b> Round 1.</li>
                <li><b>July 15 - August 14:</b> Round 2.</li>
                <li>
                    <b>August 15:</b> Inivations to present at the
                    <a href="https://iswc2022.semanticweb.org/" target="_blank"
                    >ISWC conference</a
                    >.
                </li>
                <li><b>September 15 - October 15:</b> Round 3.</li>
                <li>
                    <b>October 21</b>: Paper submissions (via easychair), and artifact
                    publication.
                </li>
                <li>
                    <b>October 23 - 27</b>: Challenge presentation during OM workshop.
                </li>
                <li>
                    <b>October 23 - 27</b>: Challenge Presentation and prize
                    announcement during ISWC.
                </li>
                <li><b>November 15</b>: Final version papers (via easychair).</li>
            </ul>

            <h3 id="datasets-track">Datasets Track</h3>

            The data that table-to-Knowledge-Graph matching systems are trained
            and evaluated on, is critical for their accuracy and relevance.<br/>
            We invite dataset submissions that provide challenging and accessible
            new datasets to advance the state-of-the-art of table-to-KG matching
            systems.<br/>
            Preferably, these datasets provide tables along with their ground
            truth annotations for at least one of CEA, CTA and CPA tasks.<br/>
            The dataset may be general or specific to a certain domain.<br/>
            <br/>
            Submissions will be evaluated according to provide the following:
            <ul>
                <li>
                    Description of the data collection, curation, and annotation
                    processes.
                </li>
                <li>
                    Availability of documentation with insights in the dataset
                    content.
                </li>
                <li>
                    Publicly accessible link to the dataset (e.g. Zenodo) and its DOI.
                </li>
                <li>Explanation of maintenance and long-term availability.</li>
                <li>Clear description of the envisioned use-cases.</li>
                <li>
                    Application in which the dataset is used to solve an exemplar
                    task.
                </li>
            </ul>

            We ask participants to describe their datasets submissions via
            easychair in a short paper (max 6 pages) that discusses how the above
            criteria are covered, while also including a link to the resources.
            The link to the resources may be private, until the submission is
            evaluated by the SemTab organisers. See paper guidelines below, for
            more details. More guidance for creating, documenting and publishing
            datasets can be found
            <a
                    href="https://neurips.cc/Conferences/2022/CallForDatasetsBenchmarks"
                    target="_blank"
            >here</a
            >.

            <br/>
            <br/>
            Important Dates (all 2022, tentative):
            <ul>
                <li>
                    <b>August 15:</b> Paper submissions (via easychair), and artifact
                    publication.
                </li>
                <li><b>September 30:</b> Notification of accept/reject.</li>
                <li>
                    <b>October 23 - 27</b>: Dataset Presentation and prize
                    announcement during ISWC.
                </li>
                <li><b>November 15</b>: Final version papers (via easychair).</li>
            </ul>

            <h3 id="artifacts-track">Artifacts Availability Badge</h3>

            New this year is the Artifacts Availability Badge which is applicable
            to the Accuracy Track as well as the Datasets Track.<br/>
            The goal of this badge is to motivate authors to publish and document
            their systems, code, and data, so that others can use these artifacts
            and potentially reproduce or build on the results.<br/>
            This badge is given if all resources are verified to satisfy the below
            criteria.<br/>
            <br/>
            The criteria used to assess submissions (both accuracy and dataset
            submissions) are:
            <ul>
                <li>Publicly accessible data (if applicable).</li>
                <li>Publicly accessible source code.</li>
                <li>Clear documentation of the code and data.</li>
                <li>Open-source dependencies.</li>
            </ul>

            <!-- BELOW FROM 2020: -->

            <!-- <p>
        The challenge will be run with the support of the
        <a href="https://www.aicrowd.com/challenges/semtab-2021" target="_blank">AICrowd platform</a> and the <a href="https://bitbucket.org/disco_unimib/stiltool/" target="_blank">STILTool system</a>.</p>
         -->

            <h3>Datasets and tasks per round</h3>

            <ul class="nav nav-tabs mb-3" id="tasks-tab" role="tablist">
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link active"
                            id="round1-cta-wd-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round1-cta-wd-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round1-cta-wd-tab-pane"
                            aria-selected="true"
                    >
                        Round 1 (CTA-WD)
                    </button>
                </li>
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link"
                            id="round1-cea-wd-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round1-cea-wd-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round1-cea-wd-tab-pane"
                            aria-selected="false"
                    >
                        Round 1 (CEA-WD)
                    </button>
                </li>
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link"
                            id="round1-cpa-wd-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round1-cpa-wd-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round1-cpa-wd-tab-pane"
                            aria-selected="false"
                    >
                        Round 1 (CPA-WD)
                    </button>
                </li>
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link"
                            id="round2-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round2-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round2-tab-pane"
                            aria-selected="false"
                            disabled
                    >
                        Round 2
                    </button>
                </li>
                <li class="nav-item" role="presentation">
                    <button
                            class="nav-link"
                            id="round3-tab"
                            data-bs-toggle="tab"
                            data-bs-target="#round3-tab-pane"
                            type="button"
                            role="tab"
                            aria-controls="round3-tab-pane"
                            aria-selected="false"
                            disabled
                    >
                        Round 3
                    </button>
                </li>
            </ul>
            <div class="tab-content" id="tasks-tab-content">
                <div
                        class="tab-pane fade show active"
                        id="round1-cta-wd-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round1-cta-wd-tab"
                        tabindex="0"
                >
                    <h3>Column Type Annotation by Wikidata (CTA-DBP)</h3>
                    <p>
                        This is a task of ISWC 2021 "Semantic Web Challenge on Tabular Data to Knowledge Graph
                        Matching".
                        It's to annotate an entity column (i.e., a column composed of entity mentions) in a table with
                        types from Wikidata (version: <a
                            href="https://dumps.wikimedia.org/wikidatawiki/entities/20210628/">20210628</a>)
                    </p>
                    <h4>Task Description</h4>
                    <p>
                        The task is to annotate each entity column by items of Wikidata as its type.
                        Each column can be annotated by multiple types:
                        the one that is as fine grained as possible and correct to all the column cells, is regarded as
                        a <strong>perfect annotation</strong>;
                        the one that is the ancestor of the perfect annotation is regarded as an <strong>okay
                        annotation</strong>;
                        others are regarded as <strong>wrong annotations</strong>.
                    </p>
                    <p>
                        The annotation can be a normal entity of Wikidata, with the prefix of
                        http://www.wikidata.org/entity/, such as http://www.wikidata.org/entity/Q8425. Each column
                        should be annotated by at most one item. A perfect annotation is encouraged with a full score,
                        while an okay annotation can still get a part of the score. Example:
                        "KIN0LD6C","0","http://www.wikidata.org/entity/Q8425". Please use the prefix of
                        http://www.wikidata.org/entity/ instead of the URL prefix https://www.wikidata.org/wiki/.
                    </p>
                    <p>
                        The annotation should be represented by its full IRI, where the case is NOT sensitive. Each
                        submission should be a CSV file. Each line should include a column identified by table id and
                        column id, and the column's annotation (a Wikidata item). It means one line should include three
                        fields: "Table ID", "Column ID" and "Annotation IRI". The headers should be excluded from the
                        submission file.
                    </p>

                    Notes:
                    <ol>
                        <li>
                            Table ID is the filename of the table data, but does NOT include the extension.
                        </li>
                        <li>
                            Column ID is the position of the column in the input, starting from 0, i.e., first column's
                            ID is 0.
                        </li>
                        <li>
                            One submission file should have NO duplicate lines for each target column.
                        </li>
                        <li>
                            Annotations for columns out of the target columns are ignored.
                        </li>
                    </ol>


                    <h4>Datasets</h4>
                    <p>
                        Table set for Round #1:
                        <a href="#">Tables</a>,
                        <a href="#">Target Columns</a>
                    </p>

                    <p>
                        Data Description: One table is stored in one CSV file. Each line corresponds to a table row. The
                        first row may either be the table header or content. The target columns for annotation are saved
                        in a CSV file
                    </p>

                    <h4>Evaluation Criteria</h4>
                    <p>
                        We encourage one perfect annotation, and at same time score one of its ancestors (okay
                        annotation). Thus we calculate Approximate Precision (\(APrecision\)), Approximate Recall
                        (\(ARecall\)), and Approximate F1 Score (\(AF1\)):

                        \[APrecision = {\sum_{a \in all\ annotations}g(a) \over all\ annotations\ \#}\]

                        \[ARecall = {\sum_{col \in all\ target\ columns}(max\_annotation\_score(col)) \over all\ target\
                        columns\ \#}\]

                        \[AF1 = {2 \times APrecision \times ARecall \over APrecision + ARecall}\]

                    </p>

                    Notes:

                    <ol>
                        <li>
                            # denotes the number.
                        </li>
                        <li>
                            \(
                            g(a) =
                            \begin{cases}
                            1.0, & \text{ if } a \text{ is a perfect annotation} \\
                            0.8^{d(a)}, & \text{ if } a \text{ is an ancestor of the perfect annotation and } d(a) < 5
                            \\
                            0.7^{d(a)}, & \text{ if } a \text{ is a descendent of the perfect annotation and } d(a) < 3
                            \\
                            0, & otherwise
                            \end{cases}
                            \)
                            <p>
                                where \(d(a)\) is the depth to the perfect annotation.
                                E.g., \(d(a)=1\) if \(a\) is a parent of the perfect annotation, and \(d(a)=2\) if \(a\)
                                is a grandparent of the perfect annotation.
                            </p>
                        </li>
                        <li>
                            \(
                            max\_annotation\_score(col) =
                            \begin{cases}
                            g(a), & \text{ if } col \text{ has an annotation } a \\
                            0, & \text{ if } col \text{ has no annotation }
                            \end{cases}

                            \)
                        </li>
                        <li>
                            \(AF1\) is used as the primary score, and \(APrecision\) is used as the secondary score.
                        </li>
                        <li>
                            A cell may have multiple equivalent Wikidata items as its GT (e.g., redirected pages
                            Q20514736 and Q852446). For an annotated entity, our evaluator will calculate the score with
                            each GT entity and select the maximum score.
                        </li>
                    </ol>

                    <h4>Submission</h4>
                    Participants can make multiple submissions per day in Round #1.
                    However, only the latest submission of each team will be evaluated on every Friday.

                    <!-- <h4>Round 1</h4>
                      <ul>
                      <li><b>Knowledge Graphs:</b> DBPedia (version: <a href="http://downloads.dbpedia.org/wiki-archive/downloads-2016-10.html">2016-10</a>) and Wikidata (version: <a href="https://doi.org/10.5281/zenodo.6153449">20210828</a>)</li>
                      <li><b>Datasets and targets:</b> <a href="./data/tables_CTA_CEA_Round1.tar.gz" target="_blank">tables of CTA-DBP and CEA-DBP</a>, <a href="./data/CTA_DBP_Round1_Targets.csv" target="_blank">CTA-DBP targets</a>, <a href="./data/CEA_DBP_Round1_Targets.csv" target="_blank">CEA-DBP targets</a>, <a href="./data/tables_CTA_CEA_WD_Round1.tar.gz" target="_blank">tables of CTA-WD and CEA-WD</a>, <a href="./data/CTA_WD_Round1_Targets.csv">CTA-WD targets</a>, <a href="./data/CEA_WD_Round1_Targets.csv">CEA-WD targets</a>.</li>
                      <li><b>CTA-DBP Task</b>: Assigning a DBPedia semantic type (a DBpedia class as fine-grained as possible) to a column. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/column-type-annotation-by-dbpedia-cta-dbp">See AIcrowd page</a>.</li>
                      <li><b>CEA-DBP Task</b>: Matching a cell to a DBpedia entity. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/cell-entity-annotation-by-dbpedia-cea-dbp">See AIcrowd page</a>.</li>
                      <li><b>CTA-WD Task</b>: Assigning a Wikidata semantic type (a Wikidata entity as fine-grained as possible) to a column. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/column-type-annotation-by-wikidata-cta-wd">See AIcrowd page</a>.</li></li>
                      <li><b>CEA-WD Task</b>: Matching a cell to a Wikidata entity. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/cell-entity-annotation-by-wikidata-cea-wd">See AIcrowd page</a></li>
                      </ul> -->
                </div>
                <div
                        class="tab-pane fade"
                        id="round1-cea-wd-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round1-cea-wd-tab"
                        tabindex="0"
                >
                    TBD
                </div>
                <div
                        class="tab-pane fade"
                        id="round1-cpa-wd-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round1-cpa-wd-tab"
                        tabindex="0"
                >
                    TBD
                </div>
                <div
                        class="tab-pane fade"
                        id="round2-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round2-tab"
                        tabindex="0"
                >
                    TBD
                </div>
                <div
                        class="tab-pane fade"
                        id="round3-tab-pane"
                        role="tabpanel"
                        aria-labelledby="round3-tab"
                        tabindex="0"
                >
                    TBD
                </div>
            </div>
            <!--
        <h4>Round 2: </h4>
        <ul>
        <li><b>Knowledge Graphs:</b> Wikidata (version: <a href="https://doi.org/10.5281/zenodo.6153449">20210828</a>)</li>
        <li><b>BioTable Datasets and targets:</b> <a href="./data/BioTable_CTA_CEA_CPA_WD_Round2.tar.gz" target="_blank">tables of BioTable-CTA-WD, BioTable-CEA-WD and BioTable-CPA-WD</a>, <a href="./data/BioTable_CTA_WD_Round2_Targets.csv" target="_blank">BioTable-CTA-WD targets</a>, <a href="./data/BioTable_CEA_WD_Round2_Targets.csv" target="_blank">BioTable-CEA-WD targets</a>, <a href="./data/BioTable_CPA_WD_Round2_Targets.csv" target="_blank">BioTable-CPA-WD targets</a>.</li>
        <li><b>BioTable-CTA-WD Task</b>: Assigning a Wikidata semantic type (a Wikidata entity as fine-grained as possible) to a column. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/biotable-column-type-annotation-by-wikidata-biotable-cta-wd">See AIcrowd page</a>.</li>
        <li><b>BioTable-CEA-WD Task</b>: Assigning a Wikidata entity to a cell. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/biotable-cell-entity-annotation-by-wikidata-biotable-cea-wd">See AIcrowd page</a>.</li>
        <li><b>BioTable-CPA-WD Task</b>: Assigning a Wikidata property to a column pair (order matters). <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/biotable-column-property-annotation-by-wikidata-biotable-cpa-wd">See AIcrowd page</a>.</li>
        <li><b>AG (HardTable) Datasets and targets:</b> <a href="./data/HardTable_CTA_CEA_CPA_WD_Round2.tar.gz" target="_blank">tables of HardTable-CTA-WD, HardTable-CEA-WD and HardTable-CPA-WD</a>, <a href="./data/HardTable_CTA_WD_Round2_Targets.csv" target="_blank">HardTable-CTA-WD targets</a>, <a href="./data/HardTable_CEA_WD_Round2_Targets.csv" target="_blank">HardTable-CEA-WD targets</a>, <a href="./data/HardTable_CPA_WD_Round2_Targets.csv" target="_blank">HardTable-CPA-WD targets</a>.</li>
        <li><b>HardTable-CTA-WD Task</b>: Assigning a Wikidata semantic type (a Wikidata entity as fine-grained as possible) to a column. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/hardtable-column-type-annotation-by-wikidata-hardtable-cta-wd">See AIcrowd page</a>.</li>
        <li><b>HardTable-CEA-WD Task</b>: Assigning a Wikidata entity to a cell. <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/hardtable-cell-entity-annotation-by-wikidata-hardtable-cea-wd">See AIcrowd page</a>.</li>
        <li><b>HardTable-CPA-WD Task</b>: Assigning a Wikidata property to a column pair (order matters). <a href="https://www.aicrowd.com/challenges/semtab-2021/problems/hardtable-column-property-annotation-by-wikidata-hardtable-cpa-wd">See AIcrowd page</a>.</li>
        </ul>

        <br>

        <h4> Round 3: </h4>
        <ul>
        <li><b>Knowledge Graphs:</b> Schema.org (version: <a href="https://gittables.github.io/">May 2021</a>),  DBPedia (version: <a href="http://downloads.dbpedia.org/wiki-archive/downloads-2016-10.html">2016-10</a>), Wikidata (version: <a href="https://doi.org/10.5281/zenodo.6153449">20210828</a>)</li>
        <li><b>BioDivTab Datasets and targets:</b> <a href="./data/BioDivTab_CTA_CEA_WD_Round3.tar.gz" target="_blank">tables of BioDivTab-CTA-WD and BioDivTab-CEA-WD</a>, <a href="./data/BioDivTab_CTA_WD_Round3_Targets.csv" target="_blank">BioDivTab-CTA-WD targets</a>, <a href="./data/BioDivTab_CEA_WD_Round3_Targets.csv" target="_blank">BioDivTab-CEA-WD targets</a>. (Knowledge Graph: "Live Edition" of Wikidata)</li>
        <li><b>GitTables Datasets and targets:</b> <a href="data/GitTables_CTA_DBP_SCH_Round3.tar.gz" target="_blank">tables of GitTables-CTA-DBP and GitTables-CTA-SCH (one column by one Schema.org class such as schema:author and schema:URL)</a>, <a href="./data/GitTables_CTA_DBP_Round3_Targets.csv" target="_blank">GitTables-CTA-DBP targets</a>, <a href="./data/GitTables_CTA_DBP_Round3_Labels.csv">GitTables-CTA-DBP labels (including DBpedia properties)</a>, <a href="data/GitTables_CTA_SCH_Round3_Targets.csv" target="_blank">GitTables-CTA-SCH targets</a>, <a href="./data/GitTables_CTA_SCH_Round3_Labels.csv">GitTables-CTA-SCH labels (including properties and types from Schema.org)</a> </li>
        <li><b>AG (HardTable) R3 Datasets and targets:</b> <a href="./data/HardTablesR3_CTA_CEA_CPA_WD_Round3.tar.gz" target="_blank">tables of HardTablesR3-CTA-WD, HardTablesR3-CEA-WD and HardTablesR3-CPA-WD</a>, <a href="./data/HardTablesR3_CTA_WD_Round3_Targets.csv" target="_blank">HardTablesR3-CTA-WD targets</a>, <a href="./data/HardTablesR3_CEA_WD_Round3_Targets.csv" target="_blank">HardTablesR3-CEA-WD targets</a>, <a href="./data/HardTablesR3_CPA_WD_Round3_Targets.csv" target="_blank">HardTablesR3-CPA-WD targets</a>. (Knowledge Graph: Wikidata (version: <a href="https://dumps.wikimedia.org/wikidatawiki/entities/20210823/">20210823</a>))</li>
        </ul> -->

            <!-- <h3>Usability Track</h3>
        This new track addresses a pain point in the community regarding a lack of publicly available easy-to-use and generic solution that will address the needs of a variety of applications and settings.
        We will devise a clear scoring mechanism to rank every participant's solution in terms of several usability criteria as judged by a review panel, for example:
        <ol>
        <li>Is the solution open-source?</li>
        <li>Does the solution require specific platform that could affect its use in common settings?</li>
        <li>Does the solution require extensive training and tuning for a new application/domain?</li>
        <li>Is the solution offered as a public service?</li>
        <li>Does the solution include a well-designed user interface?</li>
        </ol> -->

            <!-- <h3>Applications Track</h3>

        <p>This new track aims at addressing applications in real-world settings that take advantage of the output of the matching systems.
        Challenging dataset proposals are also more than welcome.</p>


        Examples include but are not limited to:
        <ol>
        <li>Applications in generic data discovery and exploration.</li>
        <li>Applications of table understanding for scientific corpora.</li>
        <li>Applications in feature engineering and automated machine learning.</li>
        </ol> -->

            <!-- <p>
        <b>Bio-Track:</b> Due to advances in biological research techniques, new data is constantly being produced in the biomedical domain and it is commonly published unstructured or tabular formats. This data is not trivial to integrate semantically due not only to its sheer amount but also the complexity of the biological relations between entities. Specifically, for tabular data annotation, the representation of data can have a significant impact in performance since each entity can be represented by alphanumeric codes (e.g., chemical formulas or gene names) or even have multiple synonyms. Therefore, the domain would greatly benefit from automated methods to map entities, entity types and properties to existing datasets to speed-up the process of integrating new data in the domain.
        </p> -->
            <!--
            <br /> -->

            <!--
        <li><b>October 21:</b> System paper submissions (via <a href="https://easychair.org/conferences/?conf=semtab2022" target="_blank">easychair</a>).</li>
        <li><b>October 25:</b> <a href="http://om2022.ontologymatching.org/" target="_blank">Ontology Matching workshop</a>.</li>
        <li><b>October 23-27:</b> <a href="https://iswc2022.semanticweb.org/" target="_blank">Challenge Presentation</a> and prize announcement.</li>
        <li><b>November 15:</b> Final version system papers (via <a href="https://easychair.org/conferences/?conf=semtab2022" target="_blank">easychair</a>).</li>
         -->

            <h2 class="pt-5" id="paper">Paper guidelines</h2>
            <hr/>

            <p>
                We invite participants in the Accuracy Track as well as the Datasets
                Track to submit a paper using easychair.<br/>
                System papers in the Accuracy Track should be no more than 12 pages
                long (excluding references) and papers for the Datasets Track are
                limited to 6 pages.<br/>
                Both type of papers should be formatted using the
                <a
                        href="https://www.overleaf.com/latex/templates/template-for-submissions-to-ceur-workshop-proceedings-ceur-ws-dot-org/wqyfdgftmcfw"
                        target="_blank"
                >CEUR Latex template</a
                >
                or the
                <a
                        href="https://ceurws.wordpress.com/2020/03/31/ceurws-publishes-ceurart-paper-style/"
                        target="_blank"
                >CEUR Word template</a
                >. Papers will be reviewed by 1-2 challenge organisers.
            </p>

            <p>
                Accepted papers will be published as a volume of
                <a href="http://ceur-ws.org/" target="_blank">CEUR-WS</a>. By
                submitting a paper, the authors accept the CEUR-WS publishing rules.
            </p>

            <h2 id="organisation" class="pt-5">Organisation</h2>
            <hr/>

            <p>
                This challenge is organised by
                <a
                        href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-Kavitha.Srinivas"
                        target="_blank"
                >Kavitha Srinivas</a
                >
                (IBM Research),
                <a
                        href="https://www.city.ac.uk/people/academics/ernesto-jimenez-ruiz"
                        target="_blank"
                >Ernesto Jim&eacute;nez-Ruiz</a
                >
                (City, University of London; University of Oslo),
                <a
                        href="https://researcher.watson.ibm.com/researcher/view.php?person=us-hassanzadeh"
                        target="_blank"
                >Oktie Hassanzadeh</a
                >
                (IBM Research),
                <a
                        href="https://www.cs.ox.ac.uk/people/jiaoyan.chen/"
                        target="_blank"
                >Jiaoyan Chen</a
                >
                (University of Oxford),
                <a href="https://sites.google.com/site/vefthym/" target="_blank"
                >Vasilis Efthymiou</a
                >
                (FORTH - ICS),
                <a href="https://vcutrona.github.io/" target="_blank"
                >Vincenzo Cutrona</a
                >
                (SUPSI),
                <a href="https://juansequeda.com/" target="_blank">Juan Sequeda</a>
                (data.world),
                <!--<a href="https://danielapoliveira.github.io/" target="_blank">Daniela Oliveira</a> (Universidade de Lisboa),
          <a href="http://www.di.fc.ul.pt/~catiapesquita/" target="_blank">Catia Pesquita</a> (Universidade de Lisboa),
          -->
                <a
                        href="https://fusion.cs.uni-jena.de/fusion/members/nora-abdelmageed/"
                        target="_blank"
                >Nora Abdelmageed</a
                >
                (University of Jena), and
                <a href="https://madelonhulsebos.github.io/" target="_blank"
                >Madelon Hulsebos</a
                >
                (Sigma Computing, University of Amsterdam). If you have any problems
                working with the datasets or any suggestions related to this
                challenge, do not hesitate to contact us via the
                <a href="https://groups.google.com/g/sem-tab-challenge"
                >discussion group</a
                >.
            </p>

            <h2 id="acknowledgements" class="pt-5">Acknowledgements</h2>
            <hr/>

            <p>
                The challenge is currently supported by the
                <a href="http://sirius-labs.no/" target="_blank"
                >SIRIUS Centre for Research-driven Innovation</a
                >
                and
                <a href="http://www.research.ibm.com/" target="_blank"
                >IBM Research</a
                >.
            </p>

            <!--<p>-->
            <!--BiodivTab is credited to Nora Abdelmageed, Sirko Schindler, Birgitta K&ouml;nig-Ries, Heinz Nixdorf Chair for Distributed Information Systems, Friedrich Schiller University Jena, Germany.-->
            <!--The tables provided in this challenge are based on real biodiversity research datasets, but have been adapted for the challenge. In the form provided here, they may be used for the challenge, only. -->
            <!--</p>-->

            <!--<p>-->
            <!--Any publication on challenge results needs to contain citations of the underlying datasets.  -->
            <!--</p>-->

            <!-- <br /> -->


        </div>
        <div class="col-md-2 col-3 vstack gap-3">
            <a href="http://sirius-labs.no/" target="_blank">
                <img src="logos/sirius-logo.png" class="img-fluid float-lg-end"
                     alt="sirius"/></a>

            <a href="http://www.cs.ox.ac.uk/" target="_blank">
                <img src="logos/cs-oxford.jpg" class="img-fluid float-lg-end"
                     alt="cs-oxford"/></a>

            <a
                    href="https://www.city.ac.uk/about/schools/mathematics-computer-science-engineering/computer-science"
                    target="_blank"
            >
                <img src="logos/city-logo.jpg" class="img-fluid float-lg-end"
                     alt="city-logo"/></a>

            <a href="https://www.ics.forth.gr/" target="_blank">
                <img src="logos/ics-forth.jpg" class="img-fluid float-lg-end"
                     alt="ics-forth"/></a>

            <a href="https://www.sigmacomputing.com" target="_blank">
                <img src="logos/sigma.png" class="img-fluid float-lg-end"
                     alt="sigma"/></a>

            <a href="https://indelab.org" target="_blank">
                <img src="logos/uva.png" class="img-fluid float-lg-end"
                     alt="uva"/></a>

            <a href="https://www.uni-jena.de/en" target="_blank">
                <img src="logos/jena.png" class="img-fluid float-lg-end"
                     alt="jena"/></a>

            <a href="https://iswc2021.semanticweb.org/" target="_blank">
                <img src="logos/iswc2022.png" class="img-fluid float-lg-end"
                     alt="iswc2022"/></a>
        </div>
    </div>
</div>

<script
        src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-pprn3073KE6tl6bjs2QrFaJGz5/SUsLqktiwsUTF55Jfv3qYSDhgCecCxMW52nD2"
        crossorigin="anonymous"
></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
</body>
</html>
